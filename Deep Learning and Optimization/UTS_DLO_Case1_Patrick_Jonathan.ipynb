{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Nama : Patrick Jonathan\n",
        "\n",
        "NIM : 2440064791\n",
        "\n",
        "Link Video : https://binusianorg-my.sharepoint.com/personal/patrick_jonathan001_binus_ac_id/_layouts/15/guestaccess.aspx?docid=072ccdf82267348128b73d8cd16fbc393&authkey=AZ6pxdfVRXJlMm6h2-96CkE&e=oj8Geh"
      ],
      "metadata": {
        "id": "lrxqklf9oImW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rtiycQ61Bit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "751277ab-50ab-47b0-c54c-7e87621eea69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.9.24)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download whenamancodes/fraud-detection"
      ],
      "metadata": {
        "id": "jYVFBpnv2eAq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "834e5b3a-e919-4847-fbd6-6f47a1c77daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading fraud-detection.zip to /content\n",
            " 91% 60.0M/66.0M [00:00<00:00, 113MB/s]\n",
            "100% 66.0M/66.0M [00:00<00:00, 126MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip fraud-detection.zip"
      ],
      "metadata": {
        "id": "tJ-wyPDc3JZf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6489b59-b446-475f-d3c4-be461fba1f2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  fraud-detection.zip\n",
            "  inflating: creditcard.csv          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load the neccessary Libraries and Data"
      ],
      "metadata": {
        "id": "Lv7ZmSM9JpfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import semua library yang dibutuhkan"
      ],
      "metadata": {
        "id": "Wdtem1N4vtsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "9wPgb1O750ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buat Class CreditCardDataset untuk melempar variabel kedalam Class ini dimana variabel akan diubah kedalam bentuk Tensor nantinya"
      ],
      "metadata": {
        "id": "dzLvk61owuiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CreditCardDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X # Feature data\n",
        "    self.y = y # Class\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    X = torch.Tensor(self.X[index]) #Tensor\n",
        "    y = torch.LongTensor(self.y[index, None]) #LongTensor\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)"
      ],
      "metadata": {
        "id": "Hu68e4QZm5w1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buka dan simpan file ke dalam variabel df menggunakan fungsi read_csv yang disediakan library pandas"
      ],
      "metadata": {
        "id": "7-o2ODbTyyLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('creditcard.csv')"
      ],
      "metadata": {
        "id": "l4Vm_GdWDpE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "itxm4gZjEPYZ",
        "outputId": "6dc27a23-6617-401b-e61b-de781e9f7963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
              "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
              "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
              "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
              "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
              "\n",
              "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
              "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
              "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
              "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
              "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
              "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
              "\n",
              "        V26       V27       V28  Amount  Class  \n",
              "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
              "1  0.125895 -0.008983  0.014724    2.69      0  \n",
              "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
              "3 -0.221929  0.062723  0.061458  123.50      0  \n",
              "4  0.502292  0.219422  0.215153   69.99      0  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-320d3dd1-d421-4fc0-b677-64e1b80548d1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-320d3dd1-d421-4fc0-b677-64e1b80548d1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-320d3dd1-d421-4fc0-b677-64e1b80548d1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-320d3dd1-d421-4fc0-b677-64e1b80548d1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset yang digunakan berasal dari kaggle yang berjudul Fraud Detection. Dimana dalam dataset ini kita diminta untuk mengklasifikasikan data apakah suatu data transaksi kartu kredit itu palsu atau tidak. Sehingga kita akan menggunakan metode klasifikasi untuk menyelesaikan masalah ini. Sayangnya, karena masalah kerahasiaan, kami tidak dapat memberikan fitur asli dan informasi latar belakang lainnya tentang data tersebut. Sehingga fitur V1 - V28 adalah komponen utama yang diperoleh dengan PCA (Principal Component Analysis), satu-satunya fitur yang belum diubah dengan PCA adalah 'Time' dan 'Amount'.\n",
        "\n",
        "'Time' adalah detik yang berlalu antara setiap transaksi dan transaksi pertama dalam dataset. \n",
        "\n",
        "'Amount' adalah jumlah transaksi. \n",
        "\n",
        "'Class' adalah variabel respon dimana akan bernilai 1 jika data tersebut palsu dan akan bernilai 0 jika data tersebut tidak palsu. "
      ],
      "metadata": {
        "id": "PyeyhXBLzTEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. EDA (Exploratory Data Analysis)"
      ],
      "metadata": {
        "id": "IDUhiZVzJKQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvRdMQq1JDY_",
        "outputId": "c8f8bf91-7512-4fb5-aa4c-dcf7a59090dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(284807, 31)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 284807 Data\n",
        "* 31 Feature"
      ],
      "metadata": {
        "id": "M0rlLQom57lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAIai8QyId6J",
        "outputId": "e3771fc3-71bd-41b1-b2e4-2a16a62755cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 284807 entries, 0 to 284806\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   Time    284807 non-null  float64\n",
            " 1   V1      284807 non-null  float64\n",
            " 2   V2      284807 non-null  float64\n",
            " 3   V3      284807 non-null  float64\n",
            " 4   V4      284807 non-null  float64\n",
            " 5   V5      284807 non-null  float64\n",
            " 6   V6      284807 non-null  float64\n",
            " 7   V7      284807 non-null  float64\n",
            " 8   V8      284807 non-null  float64\n",
            " 9   V9      284807 non-null  float64\n",
            " 10  V10     284807 non-null  float64\n",
            " 11  V11     284807 non-null  float64\n",
            " 12  V12     284807 non-null  float64\n",
            " 13  V13     284807 non-null  float64\n",
            " 14  V14     284807 non-null  float64\n",
            " 15  V15     284807 non-null  float64\n",
            " 16  V16     284807 non-null  float64\n",
            " 17  V17     284807 non-null  float64\n",
            " 18  V18     284807 non-null  float64\n",
            " 19  V19     284807 non-null  float64\n",
            " 20  V20     284807 non-null  float64\n",
            " 21  V21     284807 non-null  float64\n",
            " 22  V22     284807 non-null  float64\n",
            " 23  V23     284807 non-null  float64\n",
            " 24  V24     284807 non-null  float64\n",
            " 25  V25     284807 non-null  float64\n",
            " 26  V26     284807 non-null  float64\n",
            " 27  V27     284807 non-null  float64\n",
            " 28  V28     284807 non-null  float64\n",
            " 29  Amount  284807 non-null  float64\n",
            " 30  Class   284807 non-null  int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 67.4 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Type of attributes**\n",
        "\n",
        "Semua atribut dari setiap fiturnya adalah numerik"
      ],
      "metadata": {
        "id": "wedUmWu5KqOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "VmZ_rGKa6HfN",
        "outputId": "817b1860-bb48-47f2-f843-2763687be8a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                Time            V1            V2            V3            V4  \\\n",
              "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
              "mean    94813.859575  1.168375e-15  3.416908e-16 -1.379537e-15  2.074095e-15   \n",
              "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
              "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
              "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
              "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
              "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
              "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
              "\n",
              "                 V5            V6            V7            V8            V9  \\\n",
              "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
              "mean   9.604066e-16  1.487313e-15 -5.556467e-16  1.213481e-16 -2.406331e-15   \n",
              "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
              "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
              "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
              "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
              "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
              "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
              "\n",
              "       ...           V21           V22           V23           V24  \\\n",
              "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
              "mean   ...  1.654067e-16 -3.568593e-16  2.578648e-16  4.473266e-15   \n",
              "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
              "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
              "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
              "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
              "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
              "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
              "\n",
              "                V25           V26           V27           V28         Amount  \\\n",
              "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
              "mean   5.340915e-16  1.683437e-15 -3.660091e-16 -1.227390e-16      88.349619   \n",
              "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
              "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
              "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
              "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
              "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
              "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
              "\n",
              "               Class  \n",
              "count  284807.000000  \n",
              "mean        0.001727  \n",
              "std         0.041527  \n",
              "min         0.000000  \n",
              "25%         0.000000  \n",
              "50%         0.000000  \n",
              "75%         0.000000  \n",
              "max         1.000000  \n",
              "\n",
              "[8 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e8cb2ae1-3817-41c2-bab0-e2de1459ac4e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>284807.000000</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>...</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>284807.000000</td>\n",
              "      <td>284807.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>94813.859575</td>\n",
              "      <td>1.168375e-15</td>\n",
              "      <td>3.416908e-16</td>\n",
              "      <td>-1.379537e-15</td>\n",
              "      <td>2.074095e-15</td>\n",
              "      <td>9.604066e-16</td>\n",
              "      <td>1.487313e-15</td>\n",
              "      <td>-5.556467e-16</td>\n",
              "      <td>1.213481e-16</td>\n",
              "      <td>-2.406331e-15</td>\n",
              "      <td>...</td>\n",
              "      <td>1.654067e-16</td>\n",
              "      <td>-3.568593e-16</td>\n",
              "      <td>2.578648e-16</td>\n",
              "      <td>4.473266e-15</td>\n",
              "      <td>5.340915e-16</td>\n",
              "      <td>1.683437e-15</td>\n",
              "      <td>-3.660091e-16</td>\n",
              "      <td>-1.227390e-16</td>\n",
              "      <td>88.349619</td>\n",
              "      <td>0.001727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>47488.145955</td>\n",
              "      <td>1.958696e+00</td>\n",
              "      <td>1.651309e+00</td>\n",
              "      <td>1.516255e+00</td>\n",
              "      <td>1.415869e+00</td>\n",
              "      <td>1.380247e+00</td>\n",
              "      <td>1.332271e+00</td>\n",
              "      <td>1.237094e+00</td>\n",
              "      <td>1.194353e+00</td>\n",
              "      <td>1.098632e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>7.345240e-01</td>\n",
              "      <td>7.257016e-01</td>\n",
              "      <td>6.244603e-01</td>\n",
              "      <td>6.056471e-01</td>\n",
              "      <td>5.212781e-01</td>\n",
              "      <td>4.822270e-01</td>\n",
              "      <td>4.036325e-01</td>\n",
              "      <td>3.300833e-01</td>\n",
              "      <td>250.120109</td>\n",
              "      <td>0.041527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-5.640751e+01</td>\n",
              "      <td>-7.271573e+01</td>\n",
              "      <td>-4.832559e+01</td>\n",
              "      <td>-5.683171e+00</td>\n",
              "      <td>-1.137433e+02</td>\n",
              "      <td>-2.616051e+01</td>\n",
              "      <td>-4.355724e+01</td>\n",
              "      <td>-7.321672e+01</td>\n",
              "      <td>-1.343407e+01</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.483038e+01</td>\n",
              "      <td>-1.093314e+01</td>\n",
              "      <td>-4.480774e+01</td>\n",
              "      <td>-2.836627e+00</td>\n",
              "      <td>-1.029540e+01</td>\n",
              "      <td>-2.604551e+00</td>\n",
              "      <td>-2.256568e+01</td>\n",
              "      <td>-1.543008e+01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>54201.500000</td>\n",
              "      <td>-9.203734e-01</td>\n",
              "      <td>-5.985499e-01</td>\n",
              "      <td>-8.903648e-01</td>\n",
              "      <td>-8.486401e-01</td>\n",
              "      <td>-6.915971e-01</td>\n",
              "      <td>-7.682956e-01</td>\n",
              "      <td>-5.540759e-01</td>\n",
              "      <td>-2.086297e-01</td>\n",
              "      <td>-6.430976e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.283949e-01</td>\n",
              "      <td>-5.423504e-01</td>\n",
              "      <td>-1.618463e-01</td>\n",
              "      <td>-3.545861e-01</td>\n",
              "      <td>-3.171451e-01</td>\n",
              "      <td>-3.269839e-01</td>\n",
              "      <td>-7.083953e-02</td>\n",
              "      <td>-5.295979e-02</td>\n",
              "      <td>5.600000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>84692.000000</td>\n",
              "      <td>1.810880e-02</td>\n",
              "      <td>6.548556e-02</td>\n",
              "      <td>1.798463e-01</td>\n",
              "      <td>-1.984653e-02</td>\n",
              "      <td>-5.433583e-02</td>\n",
              "      <td>-2.741871e-01</td>\n",
              "      <td>4.010308e-02</td>\n",
              "      <td>2.235804e-02</td>\n",
              "      <td>-5.142873e-02</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.945017e-02</td>\n",
              "      <td>6.781943e-03</td>\n",
              "      <td>-1.119293e-02</td>\n",
              "      <td>4.097606e-02</td>\n",
              "      <td>1.659350e-02</td>\n",
              "      <td>-5.213911e-02</td>\n",
              "      <td>1.342146e-03</td>\n",
              "      <td>1.124383e-02</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>139320.500000</td>\n",
              "      <td>1.315642e+00</td>\n",
              "      <td>8.037239e-01</td>\n",
              "      <td>1.027196e+00</td>\n",
              "      <td>7.433413e-01</td>\n",
              "      <td>6.119264e-01</td>\n",
              "      <td>3.985649e-01</td>\n",
              "      <td>5.704361e-01</td>\n",
              "      <td>3.273459e-01</td>\n",
              "      <td>5.971390e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>1.863772e-01</td>\n",
              "      <td>5.285536e-01</td>\n",
              "      <td>1.476421e-01</td>\n",
              "      <td>4.395266e-01</td>\n",
              "      <td>3.507156e-01</td>\n",
              "      <td>2.409522e-01</td>\n",
              "      <td>9.104512e-02</td>\n",
              "      <td>7.827995e-02</td>\n",
              "      <td>77.165000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>172792.000000</td>\n",
              "      <td>2.454930e+00</td>\n",
              "      <td>2.205773e+01</td>\n",
              "      <td>9.382558e+00</td>\n",
              "      <td>1.687534e+01</td>\n",
              "      <td>3.480167e+01</td>\n",
              "      <td>7.330163e+01</td>\n",
              "      <td>1.205895e+02</td>\n",
              "      <td>2.000721e+01</td>\n",
              "      <td>1.559499e+01</td>\n",
              "      <td>...</td>\n",
              "      <td>2.720284e+01</td>\n",
              "      <td>1.050309e+01</td>\n",
              "      <td>2.252841e+01</td>\n",
              "      <td>4.584549e+00</td>\n",
              "      <td>7.519589e+00</td>\n",
              "      <td>3.517346e+00</td>\n",
              "      <td>3.161220e+01</td>\n",
              "      <td>3.384781e+01</td>\n",
              "      <td>25691.160000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e8cb2ae1-3817-41c2-bab0-e2de1459ac4e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e8cb2ae1-3817-41c2-bab0-e2de1459ac4e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e8cb2ae1-3817-41c2-bab0-e2de1459ac4e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variable Selection**"
      ],
      "metadata": {
        "id": "jXrBiqAHK1QJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sepertinya variabel 'Time' tidak berpengaruh secara signifikan terhadap hasil klasifikasi. Karena variabel 'Time' hanya menjelaskan durasi antara setiap transkasi dan transaksi pertama dalam dataset. Sehingga variabel tersebut akan saya buang dari dataset."
      ],
      "metadata": {
        "id": "wVZauiVeEdYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['Time'], axis=1)"
      ],
      "metadata": {
        "id": "QEDdvZyvEMql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "3CdO5QSZHJlM",
        "outputId": "60e137f7-f4cf-4120-eed6-fb3fda75587d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
              "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
              "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
              "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
              "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
              "\n",
              "         V8        V9       V10  ...       V21       V22       V23       V24  \\\n",
              "0  0.098698  0.363787  0.090794  ... -0.018307  0.277838 -0.110474  0.066928   \n",
              "1  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
              "2  0.247676 -1.514654  0.207643  ...  0.247998  0.771679  0.909412 -0.689281   \n",
              "3  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
              "4 -0.270533  0.817739  0.753074  ... -0.009431  0.798278 -0.137458  0.141267   \n",
              "\n",
              "        V25       V26       V27       V28  Amount  Class  \n",
              "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
              "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
              "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
              "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
              "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
              "\n",
              "[5 rows x 30 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1532bb86-7a39-4808-8d2e-cf83b7d5524d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 30 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1532bb86-7a39-4808-8d2e-cf83b7d5524d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1532bb86-7a39-4808-8d2e-cf83b7d5524d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1532bb86-7a39-4808-8d2e-cf83b7d5524d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distribution**"
      ],
      "metadata": {
        "id": "_Cs03pPzLEZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Karena kita tidak mengetahui fitur asli dari V1-V28 karena sudah melewati tahap PCA. Maka kita hanya akan melihat distribusi dari 'Class' atau variabel responnya. "
      ],
      "metadata": {
        "id": "xmaw8kSe7Bov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.countplot(x='Class',data=df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "m96SXyakONFx",
        "outputId": "8bb170b9-9de1-4282-e631-66de2f71dfac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEGCAYAAACpXNjrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASUklEQVR4nO3df+xdd13H8eeLliH+GCuuztlOOrWa1Clla7YFfwQlbt0SU9BBNiOtuFANmxFDDIMYR4ZLNIro+DEzXFlLkDGZuBoLpRkomjjcdzjZL8m+TnBtxlrWsqFkSsfbP+7n6+6622+/HZ97b/vt85Gc3HPf53M+53OTJq+ecz7nfFNVSJLU0/OmPQBJ0uJjuEiSujNcJEndGS6SpO4MF0lSd0unPYBjxamnnlqrVq2a9jAk6bhy1113faWqlh9aN1yaVatWMTMzM+1hSNJxJcmXRtW9LCZJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6s4n9Ds657e3TXsIOgbd9Ycbpz0EaeI8c5EkdWe4SJK6M1wkSd0ZLpKk7gwXSVJ3hoskqTvDRZLUneEiSerOcJEkdWe4SJK6M1wkSd0ZLpKk7gwXSVJ3hoskqTvDRZLUneEiSerOcJEkdWe4SJK6M1wkSd0ZLpKk7gwXSVJ3YwuXJGck+XSS+5Pcl+Q3W/3tSfYkubstFw/t89Yks0m+kOTCofr6VptNctVQ/cwkn231jyQ5qdVf0L7Ptu2rxvU7JUnPNs4zl4PAm6tqDXA+cEWSNW3bu6pqbVt2ALRtlwI/CqwH3pdkSZIlwHuBi4A1wGVD/fxB6+uHgAPA5a1+OXCg1d/V2kmSJmRs4VJVj1TV59r614AHgBXz7LIBuLmq/qeq/gOYBc5ty2xVPVRV/wvcDGxIEuBngY+2/bcCrxrqa2tb/yjwytZekjQBE7nn0i5LvQz4bCtdmeTzSbYkWdZqK4CHh3bb3WqHq3838NWqOnhI/Rl9te2Pt/aHjmtzkpkkM/v27fuWfqMk6WljD5ck3wncCrypqp4Argd+EFgLPAK8c9xjOJyquqGq1lXVuuXLl09rGJK06Iw1XJI8n0GwfKiq/gqgqh6tqqeq6pvA+xlc9gLYA5wxtPvKVjtc/THglCRLD6k/o6+2/UWtvSRpAsY5WyzAjcADVfXHQ/XTh5q9Gri3rW8HLm0zvc4EVgP/DNwJrG4zw05icNN/e1UV8Gngkrb/JuC2ob42tfVLgE+19pKkCVh65CbP2U8ArwPuSXJ3q72NwWyvtUABXwR+DaCq7ktyC3A/g5lmV1TVUwBJrgR2AkuALVV1X+vvLcDNSX4P+BcGYUb7/GCSWWA/g0CSJE3I2MKlqv4RGDVDa8c8+1wLXDuivmPUflX1EE9fVhuuPwm85mjGK0nqxyf0JUndGS6SpO4MF0lSd4aLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd2MLlyRnJPl0kvuT3JfkN1v9xUl2JXmwfS5r9SS5Lslsks8nOXuor02t/YNJNg3Vz0lyT9vnuiSZ7xiSpMkY55nLQeDNVbUGOB+4Iska4Crg9qpaDdzevgNcBKxuy2bgehgEBXA1cB5wLnD1UFhcD7xhaL/1rX64Y0iSJmBs4VJVj1TV59r614AHgBXABmBra7YVeFVb3wBsq4E7gFOSnA5cCOyqqv1VdQDYBaxv206uqjuqqoBth/Q16hiSpAmYyD2XJKuAlwGfBU6rqkfapi8Dp7X1FcDDQ7vtbrX56rtH1JnnGIeOa3OSmSQz+/btO/ofJkkaaezhkuQ7gVuBN1XVE8Pb2hlHjfP48x2jqm6oqnVVtW758uXjHIYknVDGGi5Jns8gWD5UVX/Vyo+2S1q0z72tvgc4Y2j3la02X33liPp8x5AkTcA4Z4sFuBF4oKr+eGjTdmBuxtcm4Lah+sY2a+x84PF2aWsncEGSZe1G/gXAzrbtiSTnt2NtPKSvUceQJE3A0jH2/RPA64B7ktzdam8Dfh+4JcnlwJeA17ZtO4CLgVng68DrAapqf5J3AHe2dtdU1f62/kbgJuCFwMfbwjzHkCRNwNjCpar+EchhNr9yRPsCrjhMX1uALSPqM8BZI+qPjTqGJGkyfEJfktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrpbULgkuX0hNUmSAJbOtzHJtwHfDpyaZBmQtulkYMWYxyZJOk7NGy7ArwFvAr4PuIunw+UJ4D1jHJck6Tg2b7hU1Z8Cf5rkN6rq3RMakyTpOHekMxcAqurdSV4OrBrep6q2jWlckqTj2ILCJckHgR8E7gaeauUCDBdJ0rMsKFyAdcCaqqpxDkaStDgs9DmXe4HvPZqOk2xJsjfJvUO1tyfZk+Tutlw8tO2tSWaTfCHJhUP19a02m+SqofqZST7b6h9JclKrv6B9n23bVx3NuCVJ37qFhsupwP1JdibZPrccYZ+bgPUj6u+qqrVt2QGQZA1wKfCjbZ/3JVmSZAnwXuAiYA1wWWsL8Aetrx8CDgCXt/rlwIFWf1drJ0maoIVeFnv70XZcVZ85irOGDcDNVfU/wH8kmQXObdtmq+ohgCQ3AxuSPAD8LPBLrc3WNsbrW19z4/0o8J4k8ZKeJE3OQmeL/X3HY16ZZCMwA7y5qg4weCDzjqE2u3n6Ic2HD6mfB3w38NWqOjii/Yq5farqYJLHW/uvdPwNkqR5LPT1L19L8kRbnkzyVJInnsPxrmcw62wt8AjwzufQRzdJNieZSTKzb9++aQ5FkhaVBYVLVX1XVZ1cVScDLwR+EXjf0R6sqh6tqqeq6pvA+3n60tce4Iyhpitb7XD1x4BTkiw9pP6Mvtr2F7X2o8ZzQ1Wtq6p1y5cvP9qfI0k6jKN+K3IN/DVw4REbHyLJ6UNfX81gFhrAduDSNtPrTGA18M/AncDqNjPsJAY3/be3+yefBi5p+28Cbhvqa1NbvwT4lPdbJGmyFvoQ5S8MfX0eg+denjzCPh8GXsHgpZe7gauBVyRZy+ABzC8yeHcZVXVfkluA+4GDwBVV9VTr50pgJ7AE2FJV97VDvAW4OcnvAf8C3NjqNwIfbJMC9jMIJEnSBC10ttjPD60fZBAMG+bboaouG1G+cURtrv21wLUj6juAHSPqD/H0ZbXh+pPAa+YbmyRpvBY6W+z14x6IJGnxWOhssZVJPtaeuN+b5NYkK8c9OEnS8WmhN/Q/wOBG+fe15W9aTZKkZ1louCyvqg9U1cG23AQ4d1eSNNJCw+WxJL88976vJL/MYZ4dkSRpoeHyq8BrgS8zeLL+EuBXxjQmSdJxbqFTka8BNrX3gJHkxcAfMQgdSZKeYaFnLj8+FywAVbUfeNl4hiRJOt4tNFyel2TZ3Jd25rLQsx5J0glmoQHxTuCfkvxl+/4aRjxNL0kSLPwJ/W1JZhj8gS6AX6iq+8c3LEnS8WzBl7ZamBgokqQjOupX7kuSdCSGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuxhYuSbYk2Zvk3qHai5PsSvJg+1zW6klyXZLZJJ9PcvbQPpta+weTbBqqn5PknrbPdUky3zEkSZMzzjOXm4D1h9SuAm6vqtXA7e07wEXA6rZsBq6HQVAAVwPnAecCVw+FxfXAG4b2W3+EY0iSJmRs4VJVnwH2H1LeAGxt61uBVw3Vt9XAHcApSU4HLgR2VdX+qjoA7ALWt20nV9UdVVXAtkP6GnUMSdKETPqey2lV9Uhb/zJwWltfATw81G53q81X3z2iPt8xniXJ5iQzSWb27dv3HH6OJGmUqd3Qb2ccNc1jVNUNVbWuqtYtX758nEORpBPKpMPl0XZJi/a5t9X3AGcMtVvZavPVV46oz3cMSdKETDpctgNzM742AbcN1Te2WWPnA4+3S1s7gQuSLGs38i8AdrZtTyQ5v80S23hIX6OOIUmakKXj6jjJh4FXAKcm2c1g1tfvA7ckuRz4EvDa1nwHcDEwC3wdeD1AVe1P8g7gztbumqqamyTwRgYz0l4IfLwtzHMMSdKEjC1cquqyw2x65Yi2BVxxmH62AFtG1GeAs0bUHxt1DEnS5PiEviSpO8NFktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6s5wkSR1Z7hIkrozXCRJ3RkukqTuDBdJUneGiySpO8NFktSd4SJJ6m4q4ZLki0nuSXJ3kplWe3GSXUkebJ/LWj1Jrksym+TzSc4e6mdTa/9gkk1D9XNa/7Nt30z+V0rSiWuaZy4/U1Vrq2pd+34VcHtVrQZub98BLgJWt2UzcD0Mwgi4GjgPOBe4ei6QWps3DO23fvw/R5I051i6LLYB2NrWtwKvGqpvq4E7gFOSnA5cCOyqqv1VdQDYBaxv206uqjuqqoBtQ31JkiZgWuFSwCeT3JVkc6udVlWPtPUvA6e19RXAw0P77m61+eq7R9SfJcnmJDNJZvbt2/et/B5J0pClUzruT1bVniTfA+xK8m/DG6uqktS4B1FVNwA3AKxbt27sx5OkE8VUzlyqak/73At8jME9k0fbJS3a597WfA9wxtDuK1ttvvrKEXVJ0oRMPFySfEeS75pbBy4A7gW2A3MzvjYBt7X17cDGNmvsfODxdvlsJ3BBkmXtRv4FwM627Ykk57dZYhuH+pIkTcA0LoudBnyszQ5eCvxFVX0iyZ3ALUkuB74EvLa13wFcDMwCXwdeD1BV+5O8A7iztbumqva39TcCNwEvBD7eFknShEw8XKrqIeClI+qPAa8cUS/gisP0tQXYMqI+A5z1LQ9WkvScHEtTkSVJi4ThIknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0WS1J3hIknqznCRJHVnuEiSujNcJEndGS6SpO4MF0lSd4aLJKk7w0WS1J3hIknqznCRJHVnuEiSulu04ZJkfZIvJJlNctW0xyNJJ5JFGS5JlgDvBS4C1gCXJVkz3VFJ0olj6bQHMCbnArNV9RBAkpuBDcD9Ux2VNCX/ec2PTXsIOgZ9/+/eM7a+F2u4rAAeHvq+Gzjv0EZJNgOb29f/SvKFCYztRHEq8JVpD+JYkD/aNO0h6Jn8tznn6vTo5SWjios1XBakqm4Abpj2OBajJDNVtW7a45AO5b/NyViU91yAPcAZQ99XtpokaQIWa7jcCaxOcmaSk4BLge1THpMknTAW5WWxqjqY5EpgJ7AE2FJV9015WCcaLzfqWOW/zQlIVU17DJKkRWaxXhaTJE2R4SJJ6s5wUVe+dkfHqiRbkuxNcu+0x3IiMFzUja/d0THuJmD9tAdxojBc1NP/v3anqv4XmHvtjjR1VfUZYP+0x3GiMFzU06jX7qyY0lgkTZHhIknqznBRT752RxJguKgvX7sjCTBc1FFVHQTmXrvzAHCLr93RsSLJh4F/An4kye4kl097TIuZr3+RJHXnmYskqTvDRZLUneEiSerOcJEkdWe4SJK6M1ykKUjyvUluTvLvSe5KsiPJD/vGXi0Wi/LPHEvHsiQBPgZsrapLW+2lwGlTHZjUkWcu0uT9DPCNqvqzuUJV/StDL/1MsirJPyT5XFte3uqnJ/lMkruT3Jvkp5IsSXJT+35Pkt+a/E+SnskzF2nyzgLuOkKbvcDPVdWTSVYDHwbWAb8E7Kyqa9vfz/l2YC2woqrOAkhyyviGLi2M4SIdm54PvCfJWuAp4Idb/U5gS5LnA39dVXcneQj4gSTvBv4W+ORURiwN8bKYNHn3Aeccoc1vAY8CL2VwxnIS/P8fvPppBm+bvinJxqo60Nr9HfDrwJ+PZ9jSwhku0uR9CnhBks1zhSQ/zjP/XMGLgEeq6pvA64Alrd1LgEer6v0MQuTsJKcCz6uqW4HfAc6ezM+QDs/LYtKEVVUleTXwJ0neAjwJfBF401Cz9wG3JtkIfAL471Z/BfDbSb4B/BewkcFf+/xAkrn/LL517D9COgLfiixJ6s7LYpKk7gwXSVJ3hoskqTvDRZLUneEiSerOcJEkdWe4SJK6+z+NdjIPr0FA3QAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berdasarkan barplot tersebut, dapat dilihat bahwa perbedaan distribusi actual data dari variabel responnya sangat jauh"
      ],
      "metadata": {
        "id": "5ty8b1FH7ZMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking Missing Values**"
      ],
      "metadata": {
        "id": "SiULzuarU7QN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOeGLn2WHedJ",
        "outputId": "95445003-e3bd-4dd1-d83d-1d79a70b0d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "V1        0\n",
              "V2        0\n",
              "V3        0\n",
              "V4        0\n",
              "V5        0\n",
              "V6        0\n",
              "V7        0\n",
              "V8        0\n",
              "V9        0\n",
              "V10       0\n",
              "V11       0\n",
              "V12       0\n",
              "V13       0\n",
              "V14       0\n",
              "V15       0\n",
              "V16       0\n",
              "V17       0\n",
              "V18       0\n",
              "V19       0\n",
              "V20       0\n",
              "V21       0\n",
              "V22       0\n",
              "V23       0\n",
              "V24       0\n",
              "V25       0\n",
              "V26       0\n",
              "V27       0\n",
              "V28       0\n",
              "Amount    0\n",
              "Class     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berdasarkan data diatas dapat dilihat bahwa dalam dataset ini tidak terdapat missing value."
      ],
      "metadata": {
        "id": "S9skBNcr7tzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Correlation**"
      ],
      "metadata": {
        "id": "Hkgd9VainIsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr()['Class'].sort_values()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp1-GDAGVA1t",
        "outputId": "4547fd3c-98a3-41d2-e3ee-65e3da5892e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "V17      -0.326481\n",
              "V14      -0.302544\n",
              "V12      -0.260593\n",
              "V10      -0.216883\n",
              "V16      -0.196539\n",
              "V3       -0.192961\n",
              "V7       -0.187257\n",
              "V18      -0.111485\n",
              "V1       -0.101347\n",
              "V9       -0.097733\n",
              "V5       -0.094974\n",
              "V6       -0.043643\n",
              "V24      -0.007221\n",
              "V13      -0.004570\n",
              "V15      -0.004223\n",
              "V23      -0.002685\n",
              "V22       0.000805\n",
              "V25       0.003308\n",
              "V26       0.004455\n",
              "Amount    0.005632\n",
              "V28       0.009536\n",
              "V27       0.017580\n",
              "V8        0.019875\n",
              "V20       0.020090\n",
              "V19       0.034783\n",
              "V21       0.040413\n",
              "V2        0.091289\n",
              "V4        0.133447\n",
              "V11       0.154876\n",
              "Class     1.000000\n",
              "Name: Class, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kita ingin melihat korelasi antara variabel respon yaitu 'Class' dengan variabel prediktor lainnya"
      ],
      "metadata": {
        "id": "eBdvb8nP8VoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plot = df.corr()['Class'][:-1].abs().sort_values().plot(kind='bar', title='Most Correlated features')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "-hNr_38jaPcc",
        "outputId": "dce6a473-489f-4d7a-d3f4-f82b13c9828d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lalu kita visualisasikan dalam bentuk plot, sehingga dapat dilihat bahwa V17 memiliki hubungan yang paling kuat dengan variabel 'Class' jika dibandingkan dengan variabel prediktor lainnya. "
      ],
      "metadata": {
        "id": "NQImIF9J82wU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Preparation"
      ],
      "metadata": {
        "id": "2absKVk3gpsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = df.drop('Class', axis=1).values\n",
        "y = df.Class.values"
      ],
      "metadata": {
        "id": "TUlwvIWQcWF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(x)"
      ],
      "metadata": {
        "id": "6fLVW7i7hU_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kita dapat menggunakan StandardScaler untuk melakukan normalisasi data sehingga data yang digunakan nantinya tidak memiliki penyimpangan yang besar"
      ],
      "metadata": {
        "id": "TmS4UHWt9vVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selanjutnya kita split dataset menjadi train, test, dan validation dengan rasio 80% train, 10% test, dan 10% validation. Lalu kita masukan train, test, dan validation data tersebut ke dalam DataLoader dan melakukan data batching yang berguna untuk mempercepat pemrosesan data dengan jumlah yang besar. Pada data ini saya menggunakan ukuran batch 30 sampel dan mengacak dataset pada train_loader agar sampel diambil secara random. "
      ],
      "metadata": {
        "id": "3tCWSYHM-CKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "test_x, valid_x, test_y, valid_y = train_test_split(test_x, test_y, test_size=0.5, random_state=42)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "train_ds = CreditCardDataset(train_x, train_y)\n",
        "train_loader = DataLoader(train_ds, batch_size=30, shuffle=True, num_workers=0)\n",
        "\n",
        "test_ds = CreditCardDataset(test_x, test_y)\n",
        "test_loader = DataLoader(test_ds, batch_size=30, shuffle=False, num_workers=0)\n",
        "\n",
        "valid_ds = CreditCardDataset(valid_x, valid_y)\n",
        "valid_loader = DataLoader(valid_ds, batch_size=30, shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "t8c6ISWYiN5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classify(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classify, self).__init__()\n",
        "        # Input layer\n",
        "        self.fc1 = nn.Linear(29, 58)\n",
        "        # Hidden layer\n",
        "        self.fc2 = nn.Linear(58, 58) \n",
        "        # Output layer\n",
        "        self.fc3 = nn.Linear(58, 2)\n",
        "        self.bn1 = nn.BatchNorm1d(58)\n",
        "        self.bn2 = nn.BatchNorm1d(58)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.bn2(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "21ePsLlBrLxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buat Arsitektur atau Model dari Backpropagation Neural Network dengan menggunakan 29 sebagai input pada input layer karena terdapat 29 variabel prediktor. Lalu dilanjut dengan hidden layer dengan input 2 x Initial Nodes (29) sehingga didapat 58 sebagai input untuk hidden layer. Terakhir pada output layer akan menghasilkan output yaitu jumlah kelas yang akan diklasifikasikan yaitu 2 (Palsu & Tidak Palsu). Lalu kita aplikasikan fungsi relu setelah melewati input layer dan hidden layer. "
      ],
      "metadata": {
        "id": "XBtOEvreAh1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lalu kita panggil model nya dan memilih loss function dan optimizer. \n",
        "\n",
        "Loss function yang saya pilih adalah CrossEntropyLoss dan Optimizer yang saya pilih adalah Adam. Karena pada Adam Optimizer, learning rate antara parameter nya akan berbeda atau menyesuaikan (adaptive) di tiap epoch. Dimana dalam dataset ini, sudah dipastikan sebelumnya bahwa perbedaan distribusi variabel responnya sangat besar. Sehingga saya rasa Adam Optimizer merupakan optimizer yang tepat untuk digunakan dalam kasus ini."
      ],
      "metadata": {
        "id": "q1du-c8ZCZPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating the model\n",
        "model = Classify()\n",
        "\n",
        "# Choosing the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Choosing the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "FswqZS5tyves"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pertama-tama disini saya melakukan train tanpa menggunakan optimizer dengan jumlah epoch 50"
      ],
      "metadata": {
        "id": "wm8O-f2PDLXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "\n",
        "train_mean_losses = []\n",
        "valid_mean_losses = []\n",
        "\n",
        "valid_best_loss = np.inf\n",
        "\n",
        "for i in range(epochs):  \n",
        "    # training \n",
        "    train_losses = []\n",
        "    \n",
        "    print(\"=========================================================\")\n",
        "    print(\"Epoch {}\".format(i))\n",
        "    \n",
        "    for iteration, batch_data in enumerate(train_loader):\n",
        "        X_batch, y_batch = batch_data\n",
        "        \n",
        "        output = model(X_batch)\n",
        "        loss = criterion(output, y_batch.squeeze())\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        train_losses.append(loss)\n",
        "    \n",
        "    train_mean_loss = torch.mean(torch.stack(train_losses))\n",
        "    print('training loss: {:10.8f}'.format(train_mean_loss))\n",
        "    \n",
        "    train_mean_losses.append(train_mean_loss)\n",
        "\n",
        "    # validation\n",
        "    valid_losses = []\n",
        "    with torch.set_grad_enabled(False):\n",
        "        for iteration, batch_data in enumerate(valid_loader):\n",
        "            X_batch, y_batch = batch_data\n",
        "\n",
        "            output = model(X_batch)\n",
        "            loss = criterion(output, y_batch.squeeze())\n",
        "            valid_losses.append(loss)\n",
        "            \n",
        "        valid_mean_loss = torch.mean(torch.stack(valid_losses))\n",
        "        print('validation loss: {:10.8f}'.format(valid_mean_loss))\n",
        "        \n",
        "        valid_mean_losses.append(valid_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20xTnm8Se-kS",
        "outputId": "274886a5-6b3f-499f-81c5-47e6bfe536dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================\n",
            "Epoch 0\n",
            "training loss: 0.79831183\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 1\n",
            "training loss: 0.79799777\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 2\n",
            "training loss: 0.79815447\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 3\n",
            "training loss: 0.79807776\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 4\n",
            "training loss: 0.79795974\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 5\n",
            "training loss: 0.79802078\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 6\n",
            "training loss: 0.79803497\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 7\n",
            "training loss: 0.79804879\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 8\n",
            "training loss: 0.79805750\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 9\n",
            "training loss: 0.79822743\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 10\n",
            "training loss: 0.79798758\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 11\n",
            "training loss: 0.79816395\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 12\n",
            "training loss: 0.79801857\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 13\n",
            "training loss: 0.79804689\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 14\n",
            "training loss: 0.79800344\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 15\n",
            "training loss: 0.79818577\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 16\n",
            "training loss: 0.79807359\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 17\n",
            "training loss: 0.79800367\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 18\n",
            "training loss: 0.79806018\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 19\n",
            "training loss: 0.79785383\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 20\n",
            "training loss: 0.79808366\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 21\n",
            "training loss: 0.79795355\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 22\n",
            "training loss: 0.79788244\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 23\n",
            "training loss: 0.79815793\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 24\n",
            "training loss: 0.79810083\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 25\n",
            "training loss: 0.79818541\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 26\n",
            "training loss: 0.79800838\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 27\n",
            "training loss: 0.79814667\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 28\n",
            "training loss: 0.79815763\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 29\n",
            "training loss: 0.79783159\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 30\n",
            "training loss: 0.79797864\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 31\n",
            "training loss: 0.79798412\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 32\n",
            "training loss: 0.79799408\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 33\n",
            "training loss: 0.79793137\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 34\n",
            "training loss: 0.79802310\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 35\n",
            "training loss: 0.79805928\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 36\n",
            "training loss: 0.79791224\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 37\n",
            "training loss: 0.79810780\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 38\n",
            "training loss: 0.79796374\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 39\n",
            "training loss: 0.79808825\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 40\n",
            "training loss: 0.79796308\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 41\n",
            "training loss: 0.79780573\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 42\n",
            "training loss: 0.79786336\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 43\n",
            "training loss: 0.79806143\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 44\n",
            "training loss: 0.79808658\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 45\n",
            "training loss: 0.79827899\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 46\n",
            "training loss: 0.79818016\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 47\n",
            "training loss: 0.79804766\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 48\n",
            "training loss: 0.79804659\n",
            "validation loss: 0.79804325\n",
            "=========================================================\n",
            "Epoch 49\n",
            "training loss: 0.79791182\n",
            "validation loss: 0.79804325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dapat dilihat bahwa jika kita melakukan training tanpa optimizer, training serta validation loss yang dihasilkan cukup besar dan tidak menunjukkan perubahan yang signifikan pada setiap epoch nya"
      ],
      "metadata": {
        "id": "bwB14CGHEUJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lalu kita lihat hasil testing dari training tanpa optimizer"
      ],
      "metadata": {
        "id": "P-Yjbp6AEluu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = np.empty((0,2))\n",
        "with torch.no_grad():\n",
        "    for iteration, batch_data in enumerate(test_loader):\n",
        "        X_batch, y_batch = batch_data        \n",
        "        output = model(X_batch)\n",
        "        \n",
        "        test_predictions = np.append(test_predictions, output.numpy(), \n",
        "                                     axis=0)\n",
        "        \n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "test_predictions = np.array(test_predictions)\n",
        "test_predictions = np.argmax(np.array(test_predictions), axis=1)\n",
        "\n",
        "print(\"=========================================================\\n\")\n",
        "print(\"Predicted Class:\")\n",
        "print(test_predictions)\n",
        "print(\"\\nGround Truth:\")\n",
        "print(test_y)\n",
        "\n",
        "print(\"\\n=========================================================\\n\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(test_y, test_predictions))\n",
        "\n",
        "print(\"\\n=========================================================\\n\")\n",
        "accuracy = accuracy_score(test_y, test_predictions)\n",
        "print(\"Accuracy: {}\".format(accuracy))\n",
        "\n",
        "f1 = f1_score(test_y, test_predictions, average='macro')\n",
        "print(\"F1 Score: \", f1)\n",
        "\n",
        "print(\"\\n=========================================================\\n\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_y, test_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoKLVHEVeDZ7",
        "outputId": "eae08fa2-ef6f-46b4-b5f4-f5cc8e8881ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================\n",
            "\n",
            "Predicted Class:\n",
            "[0 1 0 ... 0 1 1]\n",
            "\n",
            "Ground Truth:\n",
            "[0 0 0 ... 0 0 0]\n",
            "\n",
            "=========================================================\n",
            "\n",
            "Confusion Matrix:\n",
            "[[12228 16198]\n",
            " [   38    17]]\n",
            "\n",
            "=========================================================\n",
            "\n",
            "Accuracy: 0.42993574663810963\n",
            "F1 Score:  0.3015461948971182\n",
            "\n",
            "=========================================================\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.43      0.60     28426\n",
            "           1       0.00      0.31      0.00        55\n",
            "\n",
            "    accuracy                           0.43     28481\n",
            "   macro avg       0.50      0.37      0.30     28481\n",
            "weighted avg       0.99      0.43      0.60     28481\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berdasarkan hasil testing tanpa optimizer, dapat dilihat bahwa : \n",
        "\n",
        "* Accuracy : 42.99%\n",
        "* Precision 0 : 1\n",
        "* Recall 0 : 0.43\n",
        "* F1-Score 0 : 0.60\n",
        "* Precision 1 : 0\n",
        "* Recall 1 : 0.31\n",
        "* F1-Score 1 : 0\n",
        "\n",
        "Dimana angka yang dihasilkan dapat terbilang cukup kecil.\n",
        "\n",
        "Mari kita lihat apakah prediksinya akan lebih akurat jika kita training menggunakan optimizer "
      ],
      "metadata": {
        "id": "9mD7FEBGFoJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "\n",
        "train_mean_losses = []\n",
        "valid_mean_losses = []\n",
        "\n",
        "valid_best_loss = np.inf\n",
        "\n",
        "for i in range(epochs):  \n",
        "    # training \n",
        "    train_losses = []\n",
        "    \n",
        "    print(\"=========================================================\")\n",
        "    print(\"Epoch {}\".format(i))\n",
        "    \n",
        "    for iteration, batch_data in enumerate(train_loader):\n",
        "        X_batch, y_batch = batch_data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(X_batch)\n",
        "        loss = criterion(output, y_batch.squeeze())\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_losses.append(loss)\n",
        "    \n",
        "    train_mean_loss = torch.mean(torch.stack(train_losses))\n",
        "    print('training loss: {:10.8f}'.format(train_mean_loss))\n",
        "    \n",
        "    train_mean_losses.append(train_mean_loss)\n",
        "\n",
        "    # validation\n",
        "    valid_losses = []\n",
        "    with torch.set_grad_enabled(False):\n",
        "        for iteration, batch_data in enumerate(valid_loader):\n",
        "            X_batch, y_batch = batch_data\n",
        "\n",
        "            output = model(X_batch)\n",
        "            loss = criterion(output, y_batch.squeeze())\n",
        "            valid_losses.append(loss)\n",
        "            \n",
        "        valid_mean_loss = torch.mean(torch.stack(valid_losses))\n",
        "        print('validation loss: {:10.8f}'.format(valid_mean_loss))\n",
        "        \n",
        "        valid_mean_losses.append(valid_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1qfE6he7mOW",
        "outputId": "2c656597-1484-4a9a-dc5e-d4317d146a09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================\n",
            "Epoch 0\n",
            "training loss: 0.01480264\n",
            "validation loss: 0.00542871\n",
            "=========================================================\n",
            "Epoch 1\n",
            "training loss: 0.00369153\n",
            "validation loss: 0.00284048\n",
            "=========================================================\n",
            "Epoch 2\n",
            "training loss: 0.00325660\n",
            "validation loss: 0.00287424\n",
            "=========================================================\n",
            "Epoch 3\n",
            "training loss: 0.00289571\n",
            "validation loss: 0.00292494\n",
            "=========================================================\n",
            "Epoch 4\n",
            "training loss: 0.00277837\n",
            "validation loss: 0.00321526\n",
            "=========================================================\n",
            "Epoch 5\n",
            "training loss: 0.00265683\n",
            "validation loss: 0.00321316\n",
            "=========================================================\n",
            "Epoch 6\n",
            "training loss: 0.00255194\n",
            "validation loss: 0.00328755\n",
            "=========================================================\n",
            "Epoch 7\n",
            "training loss: 0.00244024\n",
            "validation loss: 0.00323530\n",
            "=========================================================\n",
            "Epoch 8\n",
            "training loss: 0.00231977\n",
            "validation loss: 0.00343004\n",
            "=========================================================\n",
            "Epoch 9\n",
            "training loss: 0.00216070\n",
            "validation loss: 0.00368738\n",
            "=========================================================\n",
            "Epoch 10\n",
            "training loss: 0.00216093\n",
            "validation loss: 0.00348976\n",
            "=========================================================\n",
            "Epoch 11\n",
            "training loss: 0.00205218\n",
            "validation loss: 0.00360459\n",
            "=========================================================\n",
            "Epoch 12\n",
            "training loss: 0.00188769\n",
            "validation loss: 0.00324541\n",
            "=========================================================\n",
            "Epoch 13\n",
            "training loss: 0.00179304\n",
            "validation loss: 0.00330146\n",
            "=========================================================\n",
            "Epoch 14\n",
            "training loss: 0.00180046\n",
            "validation loss: 0.00346931\n",
            "=========================================================\n",
            "Epoch 15\n",
            "training loss: 0.00172500\n",
            "validation loss: 0.00388622\n",
            "=========================================================\n",
            "Epoch 16\n",
            "training loss: 0.00169917\n",
            "validation loss: 0.00388137\n",
            "=========================================================\n",
            "Epoch 17\n",
            "training loss: 0.00171512\n",
            "validation loss: 0.00421441\n",
            "=========================================================\n",
            "Epoch 18\n",
            "training loss: 0.00157785\n",
            "validation loss: 0.00404464\n",
            "=========================================================\n",
            "Epoch 19\n",
            "training loss: 0.00171173\n",
            "validation loss: 0.00356329\n",
            "=========================================================\n",
            "Epoch 20\n",
            "training loss: 0.00152625\n",
            "validation loss: 0.00383518\n",
            "=========================================================\n",
            "Epoch 21\n",
            "training loss: 0.00161950\n",
            "validation loss: 0.00363813\n",
            "=========================================================\n",
            "Epoch 22\n",
            "training loss: 0.00147940\n",
            "validation loss: 0.00387547\n",
            "=========================================================\n",
            "Epoch 23\n",
            "training loss: 0.00150365\n",
            "validation loss: 0.00366719\n",
            "=========================================================\n",
            "Epoch 24\n",
            "training loss: 0.00138926\n",
            "validation loss: 0.00476620\n",
            "=========================================================\n",
            "Epoch 25\n",
            "training loss: 0.00144251\n",
            "validation loss: 0.00421332\n",
            "=========================================================\n",
            "Epoch 26\n",
            "training loss: 0.00136467\n",
            "validation loss: 0.00380015\n",
            "=========================================================\n",
            "Epoch 27\n",
            "training loss: 0.00128250\n",
            "validation loss: 0.00398641\n",
            "=========================================================\n",
            "Epoch 28\n",
            "training loss: 0.00128093\n",
            "validation loss: 0.00438823\n",
            "=========================================================\n",
            "Epoch 29\n",
            "training loss: 0.00125074\n",
            "validation loss: 0.00473792\n",
            "=========================================================\n",
            "Epoch 30\n",
            "training loss: 0.00125571\n",
            "validation loss: 0.00442725\n",
            "=========================================================\n",
            "Epoch 31\n",
            "training loss: 0.00124480\n",
            "validation loss: 0.00530566\n",
            "=========================================================\n",
            "Epoch 32\n",
            "training loss: 0.00114563\n",
            "validation loss: 0.00477437\n",
            "=========================================================\n",
            "Epoch 33\n",
            "training loss: 0.00111501\n",
            "validation loss: 0.00453540\n",
            "=========================================================\n",
            "Epoch 34\n",
            "training loss: 0.00113715\n",
            "validation loss: 0.00423592\n",
            "=========================================================\n",
            "Epoch 35\n",
            "training loss: 0.00109643\n",
            "validation loss: 0.00429930\n",
            "=========================================================\n",
            "Epoch 36\n",
            "training loss: 0.00122680\n",
            "validation loss: 0.00420625\n",
            "=========================================================\n",
            "Epoch 37\n",
            "training loss: 0.00109090\n",
            "validation loss: 0.00514842\n",
            "=========================================================\n",
            "Epoch 38\n",
            "training loss: 0.00107595\n",
            "validation loss: 0.00505981\n",
            "=========================================================\n",
            "Epoch 39\n",
            "training loss: 0.00119299\n",
            "validation loss: 0.00444245\n",
            "=========================================================\n",
            "Epoch 40\n",
            "training loss: 0.00112868\n",
            "validation loss: 0.00462464\n",
            "=========================================================\n",
            "Epoch 41\n",
            "training loss: 0.00092769\n",
            "validation loss: 0.00441073\n",
            "=========================================================\n",
            "Epoch 42\n",
            "training loss: 0.00111906\n",
            "validation loss: 0.00458305\n",
            "=========================================================\n",
            "Epoch 43\n",
            "training loss: 0.00108563\n",
            "validation loss: 0.00502503\n",
            "=========================================================\n",
            "Epoch 44\n",
            "training loss: 0.00099846\n",
            "validation loss: 0.00569291\n",
            "=========================================================\n",
            "Epoch 45\n",
            "training loss: 0.00102717\n",
            "validation loss: 0.00589940\n",
            "=========================================================\n",
            "Epoch 46\n",
            "training loss: 0.00106070\n",
            "validation loss: 0.00491016\n",
            "=========================================================\n",
            "Epoch 47\n",
            "training loss: 0.00103648\n",
            "validation loss: 0.00619539\n",
            "=========================================================\n",
            "Epoch 48\n",
            "training loss: 0.00106458\n",
            "validation loss: 0.00500202\n",
            "=========================================================\n",
            "Epoch 49\n",
            "training loss: 0.00097604\n",
            "validation loss: 0.00535093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berbeda dengan yang sebelumnya, jika kita melakukan training dengan optimizer, training serta validation loss yang dihasilkan cukup kecil dan menunjukkan perubahan yang signifikan pada setiap epoch nya. Dimana semakin lama hasil loss menunjukkan angka yang lebih kecil dari sebelumnya. Hal ini dapat dilihat dari perbedaan antara training dan validation loss pada epoch pertama dengan training dan validation loss pada epoch terakhir. "
      ],
      "metadata": {
        "id": "oCEPbxgwG4Fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = np.empty((0,2))\n",
        "with torch.no_grad():\n",
        "    for iteration, batch_data in enumerate(test_loader):\n",
        "        X_batch, y_batch = batch_data        \n",
        "        output = model(X_batch)\n",
        "        \n",
        "        test_predictions = np.append(test_predictions, output.numpy(), \n",
        "                                     axis=0)\n",
        "\n",
        "test_predictions = np.array(test_predictions)\n",
        "test_predictions = np.argmax(np.array(test_predictions), axis=1)\n",
        "\n",
        "print(\"=========================================================\\n\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(test_y, test_predictions))\n",
        "\n",
        "print(\"\\n=========================================================\\n\")\n",
        "accuracy = accuracy_score(test_y, test_predictions)\n",
        "print(\"Accuracy: {}\".format(accuracy))\n",
        "\n",
        "f1 = f1_score(test_y, test_predictions, average='macro')\n",
        "print(\"F1 Score: \", f1)\n",
        "\n",
        "print(\"\\n=========================================================\\n\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_y, test_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K80ebjxgkXG",
        "outputId": "6df358c1-a490-4ffd-fa34-455e9b949419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================\n",
            "\n",
            "Confusion Matrix:\n",
            "[[28424     2]\n",
            " [   17    38]]\n",
            "\n",
            "=========================================================\n",
            "\n",
            "Accuracy: 0.9993328885923949\n",
            "F1 Score:  0.8998329435349148\n",
            "\n",
            "=========================================================\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     28426\n",
            "           1       0.95      0.69      0.80        55\n",
            "\n",
            "    accuracy                           1.00     28481\n",
            "   macro avg       0.97      0.85      0.90     28481\n",
            "weighted avg       1.00      1.00      1.00     28481\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berdasarkan hasil testing dengan menggunakan optimizer dapat dilihat bahwa : \n",
        "\n",
        "* Accuracy : 99.93%\n",
        "* Precision 0 : 1\n",
        "* Recall 0 : 1\n",
        "* F1-Score 0 : 1\n",
        "* Precision 1 : 0.95\n",
        "* Recall 1 : 0.69\n",
        "* F1-Score 1 : 0.8\n",
        "\n",
        "Mari kita bandingkan hasilnya dengann hasil testing tanpa optimizer\n",
        "\n",
        "Hasil testing tanpa optimizer :\n",
        "\n",
        "* Accuracy : 42.99%\n",
        "* Precision 0 : 1\n",
        "* Recall 0 : 0.43\n",
        "* F1-Score 0 : 0.60\n",
        "* Precision 1 : 0\n",
        "* Recall 1 : 0.31\n",
        "* F1-Score 1 : 0\n",
        "\n",
        "Jika kita lihat kedua perbandingan tersebut, dapat dilihat bahwa semua hasil testing dengan menggunakan optimizer mengungguli semua aspek dari hasil testing tanpa optimizer. Ini artinya optimizer atau hyperparameter tuning yang dilakukan sudah tepat dan bekerja dengan baik. "
      ],
      "metadata": {
        "id": "C6zOZ42bH__j"
      }
    }
  ]
}