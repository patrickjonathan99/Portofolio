{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6c16a1e6866d4bef998dffec7addbf60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b70ef2fc6774e8997fc3edc889b5a16",
              "IPY_MODEL_28519107888845e5863c6ea39ae437fa",
              "IPY_MODEL_485bb4b22bcc427381788abcaf3aceb3"
            ],
            "layout": "IPY_MODEL_cb7298541dce414bb43d405a3398fc29"
          }
        },
        "4b70ef2fc6774e8997fc3edc889b5a16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8511303b61e4dcdb57b8d4654a7c052",
            "placeholder": "​",
            "style": "IPY_MODEL_0f1ce63428fd4909b39b3316845d4e0b",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "28519107888845e5863c6ea39ae437fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82d27291e6ff41a6b051451a90ae93f0",
            "max": 435755784,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a9c08c6d71c443588b7afb979b047af",
            "value": 435755784
          }
        },
        "485bb4b22bcc427381788abcaf3aceb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68734282fad248eb9170b6aebd5990ac",
            "placeholder": "​",
            "style": "IPY_MODEL_284530ec433e4f18a3b9d9ab0c144dd9",
            "value": " 436M/436M [00:06&lt;00:00, 121MB/s]"
          }
        },
        "cb7298541dce414bb43d405a3398fc29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8511303b61e4dcdb57b8d4654a7c052": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f1ce63428fd4909b39b3316845d4e0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82d27291e6ff41a6b051451a90ae93f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a9c08c6d71c443588b7afb979b047af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68734282fad248eb9170b6aebd5990ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "284530ec433e4f18a3b9d9ab0c144dd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Nama : Patrick Jonathan\n",
        "\n",
        "## NIM : 2440064791\n",
        "\n",
        "## Video Link : https://www.youtube.com/watch?v=VRvAGi_1QX0"
      ],
      "metadata": {
        "id": "7hHKmDCmN-1Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GacLIeftDk9B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d06173ff-9769-4dd4-c0ac-ed90676c6862"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nomor 1"
      ],
      "metadata": {
        "id": "7W_u1k5TduwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Library yang dibutuhkan untuk nomor 1"
      ],
      "metadata": {
        "id": "cfZAkclZT1lG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "FmkzMu66NJ_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import data yang digunakan untuk menyelesaikan kasus nomor 1"
      ],
      "metadata": {
        "id": "R22K10-cgjcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data01 = pd.read_csv('/content/drive/MyDrive/UAS Text Mining/data_1A.csv')"
      ],
      "metadata": {
        "id": "eR3EEOtBMqxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data01.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0EK4mjg1NNhn",
        "outputId": "b983766e-c7a8-4e0a-f7c9-13cade4e2d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                               text      label\n",
              "0           0  The Theory of Everything Review Stephen Hawkin...      Books\n",
              "1           1  Computer Networks: A Top - Down Approach About...      Books\n",
              "2           2  Sajani Premium Quality Brown Wooden Coat Hange...  Household\n",
              "3           3  Bosch Lifestyle MCM3501M 800-Watt Food Process...  Household\n",
              "4           4  Secret Wish Women's Navy-Blue Towel Bathrobe (...  Household"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-997a474a-a4d8-47e5-9adf-0a5098d4dcf6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>The Theory of Everything Review Stephen Hawkin...</td>\n",
              "      <td>Books</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Computer Networks: A Top - Down Approach About...</td>\n",
              "      <td>Books</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Sajani Premium Quality Brown Wooden Coat Hange...</td>\n",
              "      <td>Household</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Bosch Lifestyle MCM3501M 800-Watt Food Process...</td>\n",
              "      <td>Household</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Secret Wish Women's Navy-Blue Towel Bathrobe (...</td>\n",
              "      <td>Household</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-997a474a-a4d8-47e5-9adf-0a5098d4dcf6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-997a474a-a4d8-47e5-9adf-0a5098d4dcf6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-997a474a-a4d8-47e5-9adf-0a5098d4dcf6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data01['text'] = data01['text'].astype(str)"
      ],
      "metadata": {
        "id": "nEjKeVxwhYV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6KNgydeNTSA",
        "outputId": "cae13d1e-2a9b-4326-df30-34d63181eb6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. Pre-processing"
      ],
      "metadata": {
        "id": "_IlKfkU-eGhx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lakukan pre-processing untuk mendapatkan set token yang sudah berada dalam\n",
        "bentuk dasar sesuai standard tata bahasa. Pada kode dibawah ini adalah fungsi untuk melakukan pre-processing dengan case folding, menghilangkan angka, menghilangkan tanda baca, dan menghilangkan spasi berlebih. Selanjutnya teks akan ditokenisasi menjadi kata-kata. Kemudian, kita akan menghilangkan stop word dalam token tersebut. Terakhir, lakukan lematisasi kata dari token yang diperoleh."
      ],
      "metadata": {
        "id": "zzR15Z5hUUKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Case folding\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove number\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Remove multiple space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Tokenisasi teks menjadi kata-kata\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Hapus stop words\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lematisasi kata\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "BhGdtZQQe3OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Terapkan fungsi text pre-processing yang sudah dibuat pada kolom text dari data01"
      ],
      "metadata": {
        "id": "n8gyPsiTVfMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data01['text'] = data01['text'].apply(preprocess_text)\n",
        "print(data01['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1rS-EUyf8Tr",
        "outputId": "fa0f9f3a-cd55-4ab8-c938-a7d219d2fb60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        [theory, everything, review, stephen, hawking,...\n",
            "1        [computer, network, top, approach, author, beh...\n",
            "2        [sajani, premium, quality, brown, wooden, coat...\n",
            "3        [bosch, lifestyle, mcmm, watt, food, processor...\n",
            "4        [secret, wish, woman, navyblue, towel, bathrob...\n",
            "                               ...                        \n",
            "12601    [lotus, makeup, ecostay, insta, smooth, perfec...\n",
            "12602    [subtle, art, giving, fck, review, resilience,...\n",
            "12603    [elevanto, premium, collection, th, sleeve, te...\n",
            "12604    [wd, passport, tb, portable, external, hard, d...\n",
            "12605    [storite, foot, mm, male, mm, female, jack, ex...\n",
            "Name: text, Length: 12606, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berdasarkan output diatas, dapat dilihat bahwa data sudah dibersihkan dan diubah ke dalam bentuk token"
      ],
      "metadata": {
        "id": "i8xikAl4VptC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. Classification Using Machine Learning"
      ],
      "metadata": {
        "id": "RcRrzPdVef-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selanjutnya, kita lakukan vektorisasi pada kolom label agar kemudian dapat diklasifikasikan dengan menggunakan algoritma machine learning."
      ],
      "metadata": {
        "id": "wgzOhtxCWF_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "transformed_label = label_encoder.fit_transform(data01['label'])"
      ],
      "metadata": {
        "id": "Ic2xzriwtw1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selanjutnya data akan displit ke dalam train dan test dengan proporsi 80% data train dan 20% data test. Kemudian, disini saya menggunakan 2 metode vectorization yaitu CBOW (Continuous Bag-of-Words) dan skip-gram. Saya membangun kedua model tersebut dengan `vector-size` = 100, dimana setiap kata akan direpresentasikan sebagai vektor 100-dimensi. `window` diatur sebagai 5, yang berarti akan diambil 5 kata sebelum dan 5 kata sesudah kata target sebagai konteks. `min_count` diatur sebagai 1, yang berarti semua kata akan diperhitungkan.\n",
        "\n",
        "Kemudian, lakukan vektorisasi dengan mengubah token ke dalam bentuk vektor CBOW dan skip-gram."
      ],
      "metadata": {
        "id": "mtR_Jlm5WV8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data01['text'], transformed_label, test_size=0.2, random_state=42)\n",
        "\n",
        "# Membangun model Word2Vec CBOW\n",
        "w2v_cbow_model = Word2Vec(sentences=X_train, vector_size=100, window=5, min_count=1, sg=0)\n",
        "\n",
        "# Membangun model Word2Vec Skip-gram\n",
        "w2v_skipgram_model = Word2Vec(sentences=X_train, vector_size=100, window=5, min_count=1, sg=1)\n",
        "\n",
        "def get_text_vector(text, model):\n",
        "    vector = np.zeros(model.vector_size)\n",
        "    count = 0\n",
        "    for word in text:\n",
        "        if word in model.wv:\n",
        "            vector += model.wv[word]\n",
        "            count += 1\n",
        "    if count != 0:\n",
        "        vector /= count\n",
        "    return vector\n",
        "\n",
        "# Mengubah teks menjadi vektor fitur dengan Word2Vec CBOW\n",
        "X_train_vectors_cbow = [get_text_vector(text, w2v_cbow_model) for text in X_train]\n",
        "X_test_vectors_cbow = [get_text_vector(text, w2v_cbow_model) for text in X_test]\n",
        "\n",
        "# Mengubah teks menjadi vektor fitur dengan Word2Vec Skip-gram\n",
        "X_train_vectors_skipgram = [get_text_vector(text, w2v_skipgram_model) for text in X_train]\n",
        "X_test_vectors_skipgram = [get_text_vector(text, w2v_skipgram_model) for text in X_test]"
      ],
      "metadata": {
        "id": "UJPebgnFx-ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasil vektor tersebut akan dimasukkan ke dalam algoritma machine learning untuk diklasifikasi. Disini saya menggunakan 2 algoritma untuk melakukan klasifikasi yaitu, SVM (Support Vector Machine) dan Random Forest. Disini saya melakukan proses training dan menampilkan hasil akurasi prediksi pada dataset test."
      ],
      "metadata": {
        "id": "a4CP7JsWZVj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm = SVC()\n",
        "random_forest = RandomForestClassifier()\n",
        "\n",
        "# SVM dengan Word2Vec CBOW\n",
        "svm.fit(X_train_vectors_cbow, y_train)\n",
        "y_pred_svm_cbow = svm.predict(X_test_vectors_cbow)\n",
        "accuracy_svm_cbow = accuracy_score(y_test, y_pred_svm_cbow)\n",
        "print(\"Accuracy (SVM - CBOW):\", accuracy_svm_cbow)\n",
        "\n",
        "# Random Forest dengan Word2Vec CBOW\n",
        "random_forest.fit(X_train_vectors_cbow, y_train)\n",
        "y_pred_rf_cbow = random_forest.predict(X_test_vectors_cbow)\n",
        "accuracy_rf_cbow = accuracy_score(y_test, y_pred_rf_cbow)\n",
        "print(\"\\nAccuracy (Random Forest - CBOW):\", accuracy_rf_cbow)\n",
        "\n",
        "# SVM dengan Word2Vec Skip-gram\n",
        "svm.fit(X_train_vectors_skipgram, y_train)\n",
        "y_pred_svm_skipgram = svm.predict(X_test_vectors_skipgram)\n",
        "accuracy_svm_skipgram = accuracy_score(y_test, y_pred_svm_skipgram)\n",
        "print(\"\\nAccuracy (SVM - Skip-gram):\", accuracy_svm_skipgram)\n",
        "\n",
        "# Random Forest dengan Word2Vec Skip-gram\n",
        "random_forest.fit(X_train_vectors_skipgram, y_train)\n",
        "y_pred_rf_skipgram = random_forest.predict(X_test_vectors_skipgram)\n",
        "accuracy_rf_skipgram = accuracy_score(y_test, y_pred_rf_skipgram)\n",
        "print(\"\\nAccuracy (Random Forest - Skip-gram):\", accuracy_rf_skipgram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzzGJ7a486_0",
        "outputId": "6c7952b0-8955-49e9-b76d-2d15c63ff8ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (SVM - CBOW): 0.9302141157811261\n",
            "\n",
            "Accuracy (Random Forest - CBOW): 0.9460745440126883\n",
            "\n",
            "Accuracy (SVM - Skip-gram): 0.9409199048374306\n",
            "\n",
            "Accuracy (Random Forest - Skip-gram): 0.9480570975416336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berdasarkan output diatas, kita dapat melihat hasil akurasi dari algoritma yang digunakan.\n",
        "\n",
        "Jika dikaji dari segi metode vektorisasi, dapat dilihat bahwa metode skip-gram mwmberikan hasil akurasi yang lebih baik daripada metode CBOW. Hal ini dapat dilihat dari akurasi `SVM Skip-gram` (0,94) yang lebih besar dari akurasi `SVM CBOW` (0,93), serta akurasi `Random Forest Skip-gram` (0,948) yang lebih besar dari akurasi `Random Forest CBOW` (0,946).\n",
        "\n",
        "Jika dikaji dari segi Algoritma Klasifikasi, dapat dilihat bahwa algoritma Random Forest memberikan hasil akurasi yang lebih baik daripada algoritma SVM. Hal ini dapat dilihat dari akurasi `Random Forest CBOW` (0,946) yang lebih besar dari akurasi `SVM CBOW` (0,93), serta akurasi `Random Forest Skip-gram` (0,948) yang lebih besar dari akurasi `SVM Skip-gram` (0,941)."
      ],
      "metadata": {
        "id": "1CbzwggLamYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_names = np.unique(data01['label'])\n",
        "\n",
        "# Classification Report - SVM dengan Word2Vec CBOW\n",
        "print('Classification Report (SVM - Word2Vec CBOW):\\n')\n",
        "print(classification_report(y_test, y_pred_svm_cbow, target_names=target_names))\n",
        "\n",
        "# Classification Report - SVM dengan Word2Vec Skip-gram\n",
        "print('Classification Report (SVM - Word2Vec Skip-gram):\\n')\n",
        "print(classification_report(y_test, y_pred_svm_skipgram, target_names=target_names))\n",
        "\n",
        "# Classification Report - SVM dengan Word2Vec CBOW\n",
        "print('Classification Report (Random Forest - Word2Vec CBOW):\\n')\n",
        "print(classification_report(y_test, y_pred_rf_cbow, target_names=target_names))\n",
        "\n",
        "# Classification Report - SVM dengan Word2Vec Skip-gram\n",
        "print('Classification Report (Random Forest - Word2Vec Skip-gram):\\n')\n",
        "print(classification_report(y_test, y_pred_rf_skipgram, target_names=target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bxx0y759ZQHx",
        "outputId": "8fccc231-297d-485a-c88a-f7cb434bb68a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (SVM - Word2Vec CBOW):\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "                 Books       0.96      0.92      0.94       586\n",
            "Clothing & Accessories       0.93      0.94      0.94       466\n",
            "           Electronics       0.92      0.90      0.91       506\n",
            "             Household       0.92      0.95      0.93       964\n",
            "\n",
            "              accuracy                           0.93      2522\n",
            "             macro avg       0.93      0.93      0.93      2522\n",
            "          weighted avg       0.93      0.93      0.93      2522\n",
            "\n",
            "Classification Report (SVM - Word2Vec Skip-gram):\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "                 Books       0.95      0.91      0.93       586\n",
            "Clothing & Accessories       0.94      0.95      0.95       466\n",
            "           Electronics       0.93      0.92      0.93       506\n",
            "             Household       0.94      0.96      0.95       964\n",
            "\n",
            "              accuracy                           0.94      2522\n",
            "             macro avg       0.94      0.94      0.94      2522\n",
            "          weighted avg       0.94      0.94      0.94      2522\n",
            "\n",
            "Classification Report (Random Forest - Word2Vec CBOW):\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "                 Books       0.98      0.94      0.96       586\n",
            "Clothing & Accessories       0.94      0.94      0.94       466\n",
            "           Electronics       0.93      0.94      0.94       506\n",
            "             Household       0.94      0.96      0.95       964\n",
            "\n",
            "              accuracy                           0.95      2522\n",
            "             macro avg       0.95      0.94      0.95      2522\n",
            "          weighted avg       0.95      0.95      0.95      2522\n",
            "\n",
            "Classification Report (Random Forest - Word2Vec Skip-gram):\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "                 Books       0.97      0.93      0.95       586\n",
            "Clothing & Accessories       0.95      0.94      0.95       466\n",
            "           Electronics       0.94      0.94      0.94       506\n",
            "             Household       0.93      0.97      0.95       964\n",
            "\n",
            "              accuracy                           0.95      2522\n",
            "             macro avg       0.95      0.94      0.95      2522\n",
            "          weighted avg       0.95      0.95      0.95      2522\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berdasarkan hasil Classification Report diatas, dapat dilihat bahwa semua kategori baik Books, Clothing & Accessories, Electronics, dan Household dapat diklasifikasikan dengan baik dimana semua akurasinya berada diatas 90%. Namun dapat dilihat berdasarkan output semua algoritma diatas, kategori Books memiliki akurasi yang paling tinggi diantara semua kategori lainnya. Hal ini kemungkinan dikarenakan kategori `Books` memiliki deskripsi produk yang lebih menonjol atau lebih unik sehingga mudah diklasifikasikan sebagai kategori `Books`."
      ],
      "metadata": {
        "id": "F4bj9dh3ewoN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nomor 2"
      ],
      "metadata": {
        "id": "BFEpZJHdeyvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEgPzJfFfEho",
        "outputId": "86add668-9314-45d9-b7e2-2b17dbb2b2a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Library yang dibutuhkan untuk nomor 2"
      ],
      "metadata": {
        "id": "UXpmrAQzgNe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from transformers import BertModel\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re"
      ],
      "metadata": {
        "id": "-eVBel_Pe0OV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import data yang digunakan untuk menyelesaikan kasus nomor 2. Dalam hal ini data yang digunakan sama dengan data yang dipakai pada nomor 1"
      ],
      "metadata": {
        "id": "FI9QEbY7gTQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data02 = pd.read_csv('/content/drive/MyDrive/UAS Text Mining/data_1A.csv')"
      ],
      "metadata": {
        "id": "qPa8C_Mre9q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data02.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JDYiK7mJfc1u",
        "outputId": "c6196c55-a992-4967-ee84-2b5c7aec60c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                               text      label\n",
              "0           0  The Theory of Everything Review Stephen Hawkin...      Books\n",
              "1           1  Computer Networks: A Top - Down Approach About...      Books\n",
              "2           2  Sajani Premium Quality Brown Wooden Coat Hange...  Household\n",
              "3           3  Bosch Lifestyle MCM3501M 800-Watt Food Process...  Household\n",
              "4           4  Secret Wish Women's Navy-Blue Towel Bathrobe (...  Household"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2ba4f837-b100-47a7-874f-73eb270a9ce9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>The Theory of Everything Review Stephen Hawkin...</td>\n",
              "      <td>Books</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Computer Networks: A Top - Down Approach About...</td>\n",
              "      <td>Books</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Sajani Premium Quality Brown Wooden Coat Hange...</td>\n",
              "      <td>Household</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Bosch Lifestyle MCM3501M 800-Watt Food Process...</td>\n",
              "      <td>Household</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Secret Wish Women's Navy-Blue Towel Bathrobe (...</td>\n",
              "      <td>Household</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2ba4f837-b100-47a7-874f-73eb270a9ce9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2ba4f837-b100-47a7-874f-73eb270a9ce9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2ba4f837-b100-47a7-874f-73eb270a9ce9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data02['text'] = data02['text'].astype(str)"
      ],
      "metadata": {
        "id": "LY5a9_3Dfg9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lakukan pre-processing untuk mendapatkan set token yang sudah berada dalam\n",
        "bentuk dasar sesuai standard tata bahasa. Pada kode dibawah ini adalah fungsi untuk melakukan pre-processing dengan case folding, menghilangkan angka, menghilangkan tanda baca, dan menghilangkan spasi berlebih. Selanjutnya teks akan ditokenisasi menjadi kata-kata. Kemudian, kita akan menghilangkan stop word dalam token tersebut. Lalu, lakukan lematisasi kata dari token yang diperoleh. Terakhir, gabungkan kembali token menjadi kalimat, sehingga kita dapatkan kalimat yang sudah dibersihkan"
      ],
      "metadata": {
        "id": "G6_NHDNyhRO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Case folding\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove number\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Remove multiple space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Tokenisasi teks menjadi kata-kata\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Hapus stop words\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lematisasi kata\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Gabungkan kembali kata-kata menjadi kalimat\n",
        "    processed_text = ' '.join(tokens)\n",
        "\n",
        "    return processed_text"
      ],
      "metadata": {
        "id": "vGpEzQygfsJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data02['text'] = data02['text'].apply(preprocess_text)\n",
        "print(data02['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiKLvhzaf7O3",
        "outputId": "3ba86d57-ba56-432d-a039-e415e637a43f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        theory everything review stephen hawking theor...\n",
            "1        computer network top approach author behrouz f...\n",
            "2        sajani premium quality brown wooden coat hange...\n",
            "3        bosch lifestyle mcmm watt food processor black...\n",
            "4        secret wish woman navyblue towel bathrobe free...\n",
            "                               ...                        \n",
            "12601    lotus makeup ecostay insta smooth perfecting p...\n",
            "12602    subtle art giving fck review resilience happin...\n",
            "12603    elevanto premium collection th sleeve terry co...\n",
            "12604    wd passport tb portable external hard drive ye...\n",
            "12605    storite foot mm male mm female jack extension ...\n",
            "Name: text, Length: 12606, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output diatas merupakan text dari data02 yang sudah di pre-process dan siap dipakai"
      ],
      "metadata": {
        "id": "xMqzp2F7hjHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model LLM yang saya gunakan untuk menyelesaikan kasus ini adalah BERT (Bidirectional Encoder Representations from Transformers). BERT adalah salah satu model yang sangat populer dan efektif dalam pemrosesan bahasa alami (Natural Language Processing/NLP) dan pemahaman teks. Modeling dengan BERT (Bidirectional Encoder Representations from Transformers) menggunakan arsitektur transformer untuk mempelajari representasi kata yang kontekstual. BERT dapat memahami hubungan kata dalam kalimat dengan melibatkan konteks sebelum dan sesudahnya. Keunggulan BERT terletak pada representasi yang lebih baik dan kemampuan fine-tuning untuk tugas klasifikasi khusus."
      ],
      "metadata": {
        "id": "uDtCJbRsjT9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pertama-tama define tokenizer yang digunakan yaitu BERT Tokenizer yang sudah di pre-trained sebelumnya. Selain itu, define juga label yang akan digunakan."
      ],
      "metadata": {
        "id": "z0wCcpa2kgzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "labels = {'Household':0,\n",
        "          'Books':1,\n",
        "          'Electronics':2,\n",
        "          'Clothing & Accessories':3,\n",
        "          }"
      ],
      "metadata": {
        "id": "h0IdG23hgM_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selanjutnya, buat fungsi `Dataset` untuk membantu dalam mengatur dan memfasilitasi akses data dalam batch, baik untuk teks maupun label."
      ],
      "metadata": {
        "id": "I4H5hWvpkuqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "\n",
        "        self.labels = [labels[label] for label in df['label']]\n",
        "        self.texts = [tokenizer(text, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\") for text in df['text']]\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y"
      ],
      "metadata": {
        "id": "faVqZBEghU0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selanjutnya split data ke dalam train, test dan validation dengan proporsi 80% data train, 10% data test, dan 10% data valid."
      ],
      "metadata": {
        "id": "4u6zG1_2lgj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(99)\n",
        "df_train, df_val, df_test = np.split(data02.sample(frac=1, random_state=42), [int(.8*len(data02)), int(.9*len(data02))])\n",
        "\n",
        "print(len(df_train),len(df_val), len(df_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5Lqfk1Zhq8I",
        "outputId": "12b59d24-81b7-4004-f2ca-f09846d36c24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10084 1261 1261\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buat model BERT dari hasil pretrained dengan nilai dropout 0.5 dimana inputnya 768 dan output yang diharapkan 4 untuk mengklasifikasikan keempat kategori yaitu Books, Clothing & Accessories, Electronics, dan Household"
      ],
      "metadata": {
        "id": "-0zY4aYnvl_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout=0.5):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 4)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "\n",
        "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        final_layer = self.relu(linear_output)\n",
        "\n",
        "        return final_layer"
      ],
      "metadata": {
        "id": "FGsrfnreiMOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disini saya melakukan training dengan melakukan tuning pada beberapa parameter seperti :\n",
        "* `batch_size` = Batch size yang besar dapat mempercepat proses training karena lebih banyak sampel yang diproses secara paralel dalam satu iterasi. Namun penggunaan batch size yang lebih kecil dapat memperbarui bobot lebih sering, dan ini dapat membantu dalam memperbaiki model yang kompleks atau dalam menghadapi dataset yang rumit. Disini saya menggunakan 16 jumlah batch size, dimana angka ini cukup besar untuk mempercepat proses training, namun juga cukup kecil untuk mengoptimalkan hasil training model. Jika Anda ingin mendapatkan hasil training model yang lebih optimal sangat disarankan untuk menurunkan jumlah batch, namun perlu diketahui bahwa ukuran batch yang lebih besar atau lebih kecil dapat memberikan performa yang baik tergantung pada dataset dan tugas yang Anda hadapi.\n",
        "* `epoch` = Dengan menggunakan jumlah epoch yang lebih besar, model memiliki lebih banyak kesempatan untuk melihat dan mempelajari pola pada data pelatihan. Ini dapat membantu model untuk mengoptimalkan parameter dan meningkatkan kinerjanya seiring berjalannya waktu. Pada kesempatan kali ini saya menggunakan 5 epoch untuk training. Jika Anda ingin mendapatkan hasil training model yang lebih optimal sangat disarankan untuk menambah jumlah epoch.\n",
        "* `learning_rate` = Learning rate mengontrol seberapa besar langkah yang diambil pada setiap pembaruan parameter model selama pelatihan. Jika learning rate terlalu besar, langkah pembaruan parameter dapat menjadi terlalu besar yang menyebabkan model melompati minimum lokal atau bahkan bergerak menjauhinya. Hasilnya, model mungkin tidak konvergen atau memiliki performa yang buruk. Namun, jika learning rate terlalu kecil, langkah pembaruan parameter dapat menjadi terlalu kecil dan memperlambat proses pelatihan. Pada proses training ini saya menggunakan 1e-6 sebagai learning ratenya, dimana angka ini merupakan angka yang biasanya digunakan atau disarankan untuk melakukan training model.\n",
        "\n",
        "Selain itu, disini saya menggunakan Adam Optimizer dan Cross Entropy sebagai metode untuk menghitung loss nya."
      ],
      "metadata": {
        "id": "p9uc6h1gwNBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_data, val_data, learning_rate, epochs):\n",
        "\n",
        "    train, val = Dataset(train_data), Dataset(val_data)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=16, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=16)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "            model = model.cuda()\n",
        "            criterion = criterion.cuda()\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "            total_acc_train = 0\n",
        "            total_loss_train = 0\n",
        "\n",
        "            for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "                train_label = train_label.to(device)\n",
        "                mask = train_input['attention_mask'].to(device)\n",
        "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                output = model(input_id, mask)\n",
        "\n",
        "                batch_loss = criterion(output, train_label.long())\n",
        "                total_loss_train += batch_loss.item()\n",
        "\n",
        "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
        "                total_acc_train += acc\n",
        "\n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            total_acc_val = 0\n",
        "            total_loss_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for val_input, val_label in val_dataloader:\n",
        "\n",
        "                    val_label = val_label.to(device)\n",
        "                    mask = val_input['attention_mask'].to(device)\n",
        "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                    output = model(input_id, mask)\n",
        "\n",
        "                    batch_loss = criterion(output, val_label.long())\n",
        "                    total_loss_val += batch_loss.item()\n",
        "\n",
        "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
        "                    total_acc_val += acc\n",
        "\n",
        "            print(\n",
        "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
        "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
        "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
        "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
        "\n",
        "EPOCHS = 5\n",
        "model = BertClassifier()\n",
        "LR = 1e-6\n",
        "\n",
        "train(model, df_train, df_val, LR, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309,
          "referenced_widgets": [
            "6c16a1e6866d4bef998dffec7addbf60",
            "4b70ef2fc6774e8997fc3edc889b5a16",
            "28519107888845e5863c6ea39ae437fa",
            "485bb4b22bcc427381788abcaf3aceb3",
            "cb7298541dce414bb43d405a3398fc29",
            "b8511303b61e4dcdb57b8d4654a7c052",
            "0f1ce63428fd4909b39b3316845d4e0b",
            "82d27291e6ff41a6b051451a90ae93f0",
            "8a9c08c6d71c443588b7afb979b047af",
            "68734282fad248eb9170b6aebd5990ac",
            "284530ec433e4f18a3b9d9ab0c144dd9"
          ]
        },
        "id": "zY_Ue76xjIWK",
        "outputId": "27e2d1fe-9e1a-4e5c-d00d-d2991a85cacf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c16a1e6866d4bef998dffec7addbf60"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 631/631 [14:20<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 | Train Loss:  0.075                 | Train Accuracy:  0.555                 | Val Loss:  0.045                 | Val Accuracy:  0.817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 631/631 [14:23<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 2 | Train Loss:  0.033                 | Train Accuracy:  0.881                 | Val Loss:  0.024                 | Val Accuracy:  0.938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 631/631 [14:24<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 3 | Train Loss:  0.020                 | Train Accuracy:  0.936                 | Val Loss:  0.017                 | Val Accuracy:  0.948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 631/631 [14:23<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 4 | Train Loss:  0.015                 | Train Accuracy:  0.951                 | Val Loss:  0.015                 | Val Accuracy:  0.949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 631/631 [14:24<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 5 | Train Loss:  0.012                 | Train Accuracy:  0.960                 | Val Loss:  0.014                 | Val Accuracy:  0.949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output diatas merupakan hasil akurasi dan loss dari train dan valid pada setiap epoch. Jika diperhatikan, dapat dilihat bahwa loss kian menurun pada setiap epoch, dan akurasi kian meningkat pada setiap epoch dan mencapai 96% train accuracy pada epoch ke 5 serta mencapai 95% val accuracy pada epoch ke 5. Hal ini menunjukkan bahwa model berhasil mempelajari data dengan sangat baik."
      ],
      "metadata": {
        "id": "w3oK4D2g1Y_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Terakhir, lakukan evaluasi model untuk melihat sebarapa baik model LLM tersebut melakukan klasifikasi terhadap text yang diberikan"
      ],
      "metadata": {
        "id": "TP0I5KuP2IFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_data):\n",
        "\n",
        "    test = Dataset(test_data)\n",
        "\n",
        "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=16)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "        model = model.cuda()\n",
        "\n",
        "    total_acc_test = 0\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for test_input, test_label in test_dataloader:\n",
        "\n",
        "              test_label = test_label.to(device)\n",
        "              mask = test_input['attention_mask'].to(device)\n",
        "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "              output = model(input_id, mask)\n",
        "\n",
        "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
        "              total_acc_test += acc\n",
        "\n",
        "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
        "\n",
        "evaluate(model, df_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bXD4VXelEVW",
        "outputId": "28f1300b-cd82-48ea-82a4-e044f97e3a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy:  0.940\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berdasarkan output diatas, terlihat bahwa test akurasinya menunjukkan angka sekitar 94%, ini artinya model BERT berhasil memahami dan mempelajari hubungan kata dalam kalimat yang melibatkan konteks sebelum dan sesudahnya. Hal ini terbukti dari kemampuan BERT yang luar biasa dalam melakukan proses klasifikasi label berdasarkan deskripsi produk, sehingga memberikan hasil prediksi yang baik."
      ],
      "metadata": {
        "id": "YPU8J9-02Um-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nomor 3"
      ],
      "metadata": {
        "id": "yAkGqEH44hOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Library yang dibutuhkan untuk nomor 3"
      ],
      "metadata": {
        "id": "-gyttwMh3ibH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import nltk\n",
        "import re\n",
        "import gensim\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim import corpora, models"
      ],
      "metadata": {
        "id": "H3pjF_Be4kCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import data yang digunakan untuk menyelesaikan kasus nomor 3"
      ],
      "metadata": {
        "id": "t5zZB_L13kuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data03 = pd.read_csv('/content/drive/MyDrive/UAS Text Mining/data_3A.csv')"
      ],
      "metadata": {
        "id": "1l8i1HcH41j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data03.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "L9SgwYQC44mx",
        "outputId": "50acab00-7137-4ef7-c9b7-1801762df4da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                           ABSTRACT\n",
              "0           0    Predictive models allow subject-specific inf...\n",
              "1           1    Rotation invariance and translation invarian...\n",
              "2           2    We introduce and develop the notion of spher...\n",
              "3           3    The stochastic Landau--Lifshitz--Gilbert (LL...\n",
              "4           4    Fourier-transform infra-red (FTIR) spectra o..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-78276d51-fc34-44b9-b800-46b67e05b8b2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>ABSTRACT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Predictive models allow subject-specific inf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Rotation invariance and translation invarian...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>We introduce and develop the notion of spher...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-78276d51-fc34-44b9-b800-46b67e05b8b2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-78276d51-fc34-44b9-b800-46b67e05b8b2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-78276d51-fc34-44b9-b800-46b67e05b8b2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data03['ABSTRACT'] = data03['ABSTRACT'].astype(str)"
      ],
      "metadata": {
        "id": "YQaCtaWB5BPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Saya menggunakan pendekatan Latent Dirichlet Allocation (LDA) dalam pemodelan data teks ini karena pendekatan ini tidak memerlukan label untuk mengelompokkan topik-topik dalam teks. LDA adalah metode yang digunakan untuk mengidentifikasi topik-topik yang tersembunyi dalam kumpulan dokumen tanpa adanya anotasi label. Dengan LDA, kita dapat mengungkap pola tematik yang mendasari teks dan mengorganisasikan dokumen ke dalam kelompok topik yang berkaitan. Pendekatan ini sangat berguna ketika kita ingin memahami struktur tematik dalam dataset teks tanpa memiliki informasi label yang jelas. Dengan menggunakan LDA, kita dapat mengidentifikasi topik-topik yang ada dalam data dan menghasilkan pemahaman yang lebih dalam tentang isi dan konten teks tersebut."
      ],
      "metadata": {
        "id": "AcKP5AtZ826S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lakukan pre-processing untuk mendapatkan set token yang sudah berada dalam\n",
        "bentuk dasar sesuai standard tata bahasa. Pada kode dibawah ini adalah fungsi untuk melakukan pre-processing dengan case folding, menghilangkan angka, menghilangkan tanda baca, dan menghilangkan spasi berlebih. Selanjutnya teks akan ditokenisasi menjadi kata-kata. Kemudian, kita akan menghilangkan stop word dalam token tersebut. Lalu, lakukan lematisasi kata dari token yang diperoleh. Terakhir, buat fungsi baru untuk menggabungkan kembali token menjadi kalimat, sehingga kita dapatkan kalimat yang sudah dibersihkan"
      ],
      "metadata": {
        "id": "QGFA0CsC3tlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def tokenize_text(text):\n",
        "    # Case folding\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove number\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Remove multiple space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Tokenisasi teks menjadi kata-kata\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Hapus stop words\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lematisasi kata\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def cleaned(text):\n",
        "    tokens = tokenize_text(text)\n",
        "    processed_text = ' '.join(tokens)\n",
        "\n",
        "    return processed_text"
      ],
      "metadata": {
        "id": "9lMZXFvfFkZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lakukan tokenisasi untuk mendapatkan token dari abstract yang diberikan"
      ],
      "metadata": {
        "id": "8BIXGYnG4LHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = data03['ABSTRACT'].apply(tokenize_text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8eBpAebGLKq",
        "outputId": "515eb152-e1a6-474d-93d4-55d3f31afce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0     [predictive, model, allow, subjectspecific, in...\n",
            "1     [rotation, invariance, translation, invariance...\n",
            "2     [introduce, develop, notion, spherical, polyha...\n",
            "3     [stochastic, landaulifshitzgilbert, llg, equat...\n",
            "4     [fouriertransform, infrared, ftir, spectrum, s...\n",
            "                            ...                        \n",
            "95    [paper, presented, novel, convolutional, neura...\n",
            "96    [variety, representation, learning, approach, ...\n",
            "97    [motivated, perelmans, pseudo, locality, theor...\n",
            "98    [bound, exponential, sum, appears, study, irre...\n",
            "99    [investigate, effect, dimensional, crossover, ...\n",
            "Name: ABSTRACT, Length: 100, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selanjutkan, siapkan juga cleaned text yang akan digunakan sebagai sampel untuk melakukan prediksi topik dari text yang sudah dibersihkan"
      ],
      "metadata": {
        "id": "aayl6Trv4TY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = data03['ABSTRACT'].apply(cleaned)\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1zoclJWG7zQ",
        "outputId": "f882f2af-9063-4b90-a5d9-bbb5fa39eedf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0     predictive model allow subjectspecific inferen...\n",
            "1     rotation invariance translation invariance gre...\n",
            "2     introduce develop notion spherical polyharmoni...\n",
            "3     stochastic landaulifshitzgilbert llg equation ...\n",
            "4     fouriertransform infrared ftir spectrum sample...\n",
            "                            ...                        \n",
            "95    paper presented novel convolutional neural net...\n",
            "96    variety representation learning approach inves...\n",
            "97    motivated perelmans pseudo locality theorem ri...\n",
            "98    bound exponential sum appears study irregulari...\n",
            "99    investigate effect dimensional crossover groun...\n",
            "Name: ABSTRACT, Length: 100, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selanjutnya, buat fungsi `build_dic` untuk membuat objek kamus baru menggunakan kelas Dictionary dari library Gensim dengan beberapa ketentuan seperti :\n",
        "* `no_below=10` berarti kata-kata yang muncul kurang dari 10 kali dalam keseluruhan teks akan dihapus dari kamus.\n",
        "* `no_above=0.2` berarti kata-kata yang muncul dalam lebih dari 20% teks akan dihapus dari kamus.\n",
        "* `keep_n=100000` berarti kamus akan mempertahankan 100.000 kata teratas berdasarkan frekuensinya."
      ],
      "metadata": {
        "id": "v95_C1Zk4p3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dic(text):\n",
        "    dictionary = Dictionary(text)\n",
        "    dictionary.filter_extremes(no_below=10, no_above=0.2, keep_n= 100000)\n",
        "    return dictionary"
      ],
      "metadata": {
        "id": "aC1fSwSv7dJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lalu, buat fungsi `build_vec` untuk membangun representasi vektor dari teks menggunakan model Bag-of-Words (BOW) berdasarkan kamus yang diberikan."
      ],
      "metadata": {
        "id": "2-xtr6hw5h6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vec(text,dictionary):\n",
        "    bow_corpus = [dictionary.doc2bow(doc) for doc in text]\n",
        "    return bow_corpus"
      ],
      "metadata": {
        "id": "mMqE-WVY7h6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berikutnya, buat fungsi `vector_tfidf` untuk menghitung representasi vektor TF-IDF (Term Frequency-Inverse Document Frequency) dari representasi vektor Bag-of-Words (BOW) yang diberikan"
      ],
      "metadata": {
        "id": "1ecDHdCW5x-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_tfidf(bow_corpus):\n",
        "    tfidf = models.TfidfModel(bow_corpus)\n",
        "    corpus_tfidf = tfidf[bow_corpus]\n",
        "    return corpus_tfidf"
      ],
      "metadata": {
        "id": "GknO4qU87jc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buat juga fungsi `model_lda` untuk melatih model Latent Dirichlet Allocation (LDA) menggunakan representasi vektor Bag-of-Words (BoW) dan mengembalikan model LDA yang telah dilatih."
      ],
      "metadata": {
        "id": "oj0efkzK7YYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_lda(dictionary,bow_corpus,num_topic,alpha,eta):\n",
        "    lda_model =  gensim.models.LdaMulticore(bow_corpus,\n",
        "                                   num_topics = num_topic,\n",
        "                                   id2word = dictionary,\n",
        "                                   passes = 25,\n",
        "                                   workers = 5,\n",
        "                                   alpha=alpha,\n",
        "                                   eta=eta)\n",
        "    return lda_model"
      ],
      "metadata": {
        "id": "dlg_bM4X7lAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Terakhir, buat fungsi `score_perf` untuk menghitung coherence score dari model LDA menggunakan teks dan kamus yang diberikan."
      ],
      "metadata": {
        "id": "iI2TBg4A7z6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def score_perf(lda_model,text,dictionary):\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=text, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_lda = coherence_model_lda.get_coherence()\n",
        "    return coherence_lda"
      ],
      "metadata": {
        "id": "oaJU3dTO7m5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setelah membuat semua fungsi yang dibutuhkan, lanjutkan dengan membuat dictionary dari token yang sudah dicleansing, lalu buat juga BOW Corpusnya, serta TF-IDF dari corpus yang sudah dibuat."
      ],
      "metadata": {
        "id": "D_aCV81u7-l9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = build_dic(tokens)"
      ],
      "metadata": {
        "id": "352NGGs07reO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_corpus = build_vec(tokens, dictionary)"
      ],
      "metadata": {
        "id": "sFrhB7b57s-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_tfidf = vector_tfidf(bow_corpus)"
      ],
      "metadata": {
        "id": "ye8BtH5R7uCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering 3 Topic"
      ],
      "metadata": {
        "id": "fXcmppBGVBRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kita akan melakukan LDA dengan beberapa jumlah topik, pertama-tama saya akan menggunakan 3 jumlah topik untuk melakukan clustering terlebih dahulu."
      ],
      "metadata": {
        "id": "ktdRnlG18Vjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model1 = model_lda(dictionary, corpus_tfidf, 3, 0.7, 0.7)"
      ],
      "metadata": {
        "id": "MZ_sizMc7vqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coherence_score=score_perf(lda_model1, tokens, dictionary)\n",
        "print(coherence_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRfKyOzf7xNe",
        "outputId": "a1bee8d4-f139-45c2-a31b-e54681c9f54f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2465261092173021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dapat dilihat bahwa nilai coherence score dari model1 menunjukkan angka 0.2465, dimana angka ini dapat terbilang cukup rendah. Artinya kata-kata yang diatribusikan ke dalam topik-topik tersebut tidak memiliki hubungan yang kuat atau tematik yang jelas"
      ],
      "metadata": {
        "id": "uxXQbIOb9Knf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, topic in lda_model1.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7c_MvMG7yha",
        "outputId": "1c3545b8-23b6-4ba8-adeb-68dda6f4eb13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: 0 \n",
            "Words: 0.048*\"network\" + 0.046*\"system\" + 0.038*\"approach\" + 0.037*\"learning\" + 0.032*\"framework\" + 0.031*\"performance\" + 0.031*\"task\" + 0.028*\"application\" + 0.028*\"function\" + 0.026*\"stateoftheart\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.042*\"measure\" + 0.039*\"phase\" + 0.034*\"used\" + 0.031*\"present\" + 0.029*\"function\" + 0.028*\"proposed\" + 0.028*\"parameter\" + 0.027*\"well\" + 0.026*\"two\" + 0.025*\"main\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.037*\"dynamic\" + 0.037*\"energy\" + 0.036*\"state\" + 0.034*\"finite\" + 0.033*\"equation\" + 0.030*\"domain\" + 0.027*\"condition\" + 0.026*\"solution\" + 0.026*\"first\" + 0.026*\"data\"\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output diatas merupakan informasi mengenai setiap topik yang dihasilkan oleh model LDA. Informasi ini termasuk nomor topik dan kata-kata yang paling relevan dengan topik tersebut. Misalnya pada topic 0, kata yang paling relevan dengan topic tersebut adalah `network` dengan probabilitas sebesar 0.048. Pada topic 1, kata yang paling relevan dengan topic tersebut adalah `measure` dengan probabilitas sebesar 0.042. Pada topic 2, kata yang paling relevan dengan topic tersebut adalah `dynamic` dengan probabilitas sebesar 0.037."
      ],
      "metadata": {
        "id": "UqPFKrZK-Svc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lakukan prediksi topic clustering pada sampel. Disini saya mengambil 5 sampel, yang saya ambil dari data ke 55 sampai data ke 60"
      ],
      "metadata": {
        "id": "l8uvIFD6-9Lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_token = tokens.iloc[55:60]\n",
        "X_test_cleaned = cleaned_text.iloc[55:60]"
      ],
      "metadata": {
        "id": "bXBC4mNnA0OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_doc = [i for i in X_test_cleaned]"
      ],
      "metadata": {
        "id": "HFSSNGksLNKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_corpusTest = build_vec(X_test_token, dictionary)"
      ],
      "metadata": {
        "id": "5fUCJHUSB_4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_tfidfTest = vector_tfidf(bow_corpusTest)"
      ],
      "metadata": {
        "id": "DJic72f8CEq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lakukan prediksi topik clustering menggunakan model LDA yang telah dibuat sebelumnya. Lalu tampilkan juga 2 urutan tertinggi dari score topik yang paling cocok dengan dokumen tersebut."
      ],
      "metadata": {
        "id": "eihvAKlU_TZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Top 2 The highest score of topic\n",
        "test_shape=len(X_test_doc)\n",
        "topic1=[]\n",
        "score1=[]\n",
        "topic2=[]\n",
        "score2=[]\n",
        "sentence=[]\n",
        "for doc in range(test_shape):\n",
        "    sentence.append(X_test_doc[doc])\n",
        "    topic1.append(sorted(lda_model1[bow_corpusTest[doc]], key=lambda tup: -1*tup[1])[0][0])\n",
        "    score1.append(sorted(lda_model1[bow_corpusTest[doc]], key=lambda tup: -1*tup[1])[0][1])\n",
        "    topic2.append(sorted(lda_model1[bow_corpusTest[doc]], key=lambda tup: -1*tup[1])[1][0])\n",
        "    score2.append(sorted(lda_model1[bow_corpusTest[doc]], key=lambda tup: -1*tup[1])[1][1])"
      ],
      "metadata": {
        "id": "r4oRcZfBCKzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=pd.DataFrame(sentence,columns=['sentence']).reset_index()\n",
        "topic1=pd.DataFrame(topic1,columns=['topic1']).reset_index()\n",
        "score1=pd.DataFrame(score1,columns=['score1']).reset_index()\n",
        "topic2=pd.DataFrame(topic2,columns=['topic2']).reset_index()\n",
        "score2=pd.DataFrame(score2,columns=['score2']).reset_index()\n",
        "test_result=pd.concat([sentence,topic1,score1,topic2,score2],axis=1)"
      ],
      "metadata": {
        "id": "jEFw8KhnIgtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_result1 = test_result[['sentence','topic1','score1','topic2','score2']]\n",
        "test_result1.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Kb7C-4V9IjOp",
        "outputId": "4c96f443-d180-45eb-882d-c6d7270e3b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            sentence  topic1    score1  \\\n",
              "0  investigating emergence particular cell type r...       2  0.771570   \n",
              "1  stimuliresponsive material modify shape respon...       2  0.508125   \n",
              "2  today landscape robotics dominated vertical in...       0  0.580442   \n",
              "3  machine learning model especially based deep a...       0  0.833165   \n",
              "4  study query complexity cake cutting give lower...       0  0.333333   \n",
              "\n",
              "   topic2    score2  \n",
              "0       0  0.129495  \n",
              "1       1  0.285351  \n",
              "2       1  0.300354  \n",
              "3       1  0.128918  \n",
              "4       1  0.333333  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2c8c5f8a-1ee1-450d-a527-13ac41dcbeb1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>topic1</th>\n",
              "      <th>score1</th>\n",
              "      <th>topic2</th>\n",
              "      <th>score2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>investigating emergence particular cell type r...</td>\n",
              "      <td>2</td>\n",
              "      <td>0.771570</td>\n",
              "      <td>0</td>\n",
              "      <td>0.129495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>stimuliresponsive material modify shape respon...</td>\n",
              "      <td>2</td>\n",
              "      <td>0.508125</td>\n",
              "      <td>1</td>\n",
              "      <td>0.285351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>today landscape robotics dominated vertical in...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.580442</td>\n",
              "      <td>1</td>\n",
              "      <td>0.300354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>machine learning model especially based deep a...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.833165</td>\n",
              "      <td>1</td>\n",
              "      <td>0.128918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>study query complexity cake cutting give lower...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2c8c5f8a-1ee1-450d-a527-13ac41dcbeb1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2c8c5f8a-1ee1-450d-a527-13ac41dcbeb1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2c8c5f8a-1ee1-450d-a527-13ac41dcbeb1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 284
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berdasarkan output diatas, dapat dilihat bahwa data pertama teridentifikasi sebagai topik 2 sebesar 77,16%, dan juga teridentifikasi sebagai topik 0 sebesar 12,95%, sehingga dapat disimpulkan bahwa data pertama lebih cocok untuk masuk ke dalam topik 2. Begitu juga data yang lainnya. Namun, data terakhir teridentifikasi sebagai topik 0 sebesar 33,33%, dan juga teridentifikasi sebagai topik 1 sebesar 33,33%, sehingga data terakhir dapat termasuk ke dalam topik 0, namun juga dapat termasuk ke dalam topik 1 karena memiliki tingkat probabilitas yang sama."
      ],
      "metadata": {
        "id": "ceb0Na3J_ca9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering 5 Topic"
      ],
      "metadata": {
        "id": "CEkpyR6ZVWWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selanjutnya, kita akan mencoba melakukan LDA dengan menggunakan 5 jumlah topik untuk melakukan clustering."
      ],
      "metadata": {
        "id": "mANd4M4XGl6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model2 = model_lda(dictionary, corpus_tfidf, 5, 0.7, 0.7)"
      ],
      "metadata": {
        "id": "WXERUE8dVZ0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coherence_score=score_perf(lda_model2, tokens, dictionary)\n",
        "print(coherence_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXMEK7Z_Vbh9",
        "outputId": "6d00613c-3c9c-4948-d5d9-e5d512dea0f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.22450146008606514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dapat dilihat bahwa nilai coherence score dari model2 menunjukkan angka 0.2245, dimana angka ini dapat terbilang cukup rendah. Artinya kata-kata yang diatribusikan ke dalam topik-topik tersebut tidak memiliki hubungan yang kuat atau tematik yang jelas. Angka ini juga lebih kecil dibandingkan dengan nilai coherence score dari model1, yang artinya proses clustering akan memberikan hasil yang lebih baik jika menggunakan 3 topik."
      ],
      "metadata": {
        "id": "pKtxZ-8mGxHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, topic in lda_model2.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_LB-tNEVdyi",
        "outputId": "cf332b4e-bc0d-49d2-8406-8e33e4d71f55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: 0 \n",
            "Words: 0.028*\"data\" + 0.026*\"sample\" + 0.023*\"performance\" + 0.022*\"used\" + 0.021*\"network\" + 0.021*\"learning\" + 0.021*\"task\" + 0.020*\"system\" + 0.020*\"approach\" + 0.019*\"problem\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.033*\"system\" + 0.022*\"approach\" + 0.021*\"used\" + 0.020*\"solution\" + 0.020*\"data\" + 0.019*\"network\" + 0.019*\"dynamic\" + 0.019*\"present\" + 0.019*\"equation\" + 0.019*\"set\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.021*\"dynamic\" + 0.021*\"algorithm\" + 0.021*\"effect\" + 0.020*\"state\" + 0.020*\"finite\" + 0.020*\"data\" + 0.019*\"local\" + 0.019*\"approach\" + 0.019*\"condition\" + 0.019*\"problem\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.046*\"function\" + 0.030*\"network\" + 0.023*\"measure\" + 0.019*\"system\" + 0.018*\"state\" + 0.018*\"present\" + 0.018*\"learning\" + 0.018*\"problem\" + 0.018*\"two\" + 0.017*\"approach\"\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.026*\"phase\" + 0.022*\"approach\" + 0.022*\"system\" + 0.021*\"domain\" + 0.021*\"energy\" + 0.020*\"finite\" + 0.020*\"set\" + 0.019*\"performance\" + 0.019*\"state\" + 0.019*\"measure\"\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output diatas merupakan informasi mengenai setiap topik yang dihasilkan oleh model LDA. Informasi ini termasuk nomor topik dan kata-kata yang paling relevan dengan topik tersebut. Misalnya pada topic 0, kata yang paling relevan dengan topic tersebut adalah `data` dengan probabilitas sebesar 0.028. Pada topic 1, kata yang paling relevan dengan topic tersebut adalah `system` dengan probabilitas sebesar 0.033. Pada topic 2, kata yang paling relevan dengan topic tersebut adalah `dynamic` dengan probabilitas sebesar 0.021. Pada topic 3, kata yang paling relevan dengan topic tersebut adalah `function` dengan probabilitas sebesar 0.046. Pada topic 4, kata yang paling relevan dengan topic tersebut adalah `phase` dengan probabilitas sebesar 0.026."
      ],
      "metadata": {
        "id": "NLilDiMLG3-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dengan menggunakan data testing yang sama seperti sebelumnya, lakukan prediksi topik clustering menggunakan model LDA yang telah dibuat sebelumnya. Lalu tampilkan juga 2 urutan tertinggi dari score topik yang paling cocok dengan dokumen tersebut."
      ],
      "metadata": {
        "id": "aJjK3TTvHobY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 2 The highest score of topic\n",
        "test_shape=len(X_test_doc)\n",
        "topic1=[]\n",
        "score1=[]\n",
        "topic2=[]\n",
        "score2=[]\n",
        "sentence=[]\n",
        "for doc in range(test_shape):\n",
        "    sentence.append(X_test_doc[doc])\n",
        "    topic1.append(sorted(lda_model2[bow_corpusTest[doc]], key=lambda tup: -1*tup[1])[0][0])\n",
        "    score1.append(sorted(lda_model2[bow_corpusTest[doc]], key=lambda tup: -1*tup[1])[0][1])\n",
        "    topic2.append(sorted(lda_model2[bow_corpusTest[doc]], key=lambda tup: -1*tup[1])[1][0])\n",
        "    score2.append(sorted(lda_model2[bow_corpusTest[doc]], key=lambda tup: -1*tup[1])[1][1])"
      ],
      "metadata": {
        "id": "RY0muMR5WCNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=pd.DataFrame(sentence,columns=['sentence']).reset_index()\n",
        "topic1=pd.DataFrame(topic1,columns=['topic1']).reset_index()\n",
        "score1=pd.DataFrame(score1,columns=['score1']).reset_index()\n",
        "topic2=pd.DataFrame(topic2,columns=['topic2']).reset_index()\n",
        "score2=pd.DataFrame(score2,columns=['score2']).reset_index()\n",
        "test_result=pd.concat([sentence,topic1,score1,topic2,score2],axis=1)"
      ],
      "metadata": {
        "id": "3Ojdx3x1WQL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_result2 = test_result[['sentence','topic1','score1','topic2','score2']]\n",
        "test_result2.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "atpksVqUWVBf",
        "outputId": "f2da7e84-f5bb-49e2-86b9-414f82d1f7ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            sentence  topic1    score1  \\\n",
              "0  investigating emergence particular cell type r...       2  0.406604   \n",
              "1  stimuliresponsive material modify shape respon...       3  0.282587   \n",
              "2  today landscape robotics dominated vertical in...       1  0.247078   \n",
              "3  machine learning model especially based deep a...       0  0.796006   \n",
              "4  study query complexity cake cutting give lower...       0  0.200000   \n",
              "\n",
              "   topic2    score2  \n",
              "0       4  0.180537  \n",
              "1       2  0.253033  \n",
              "2       0  0.207291  \n",
              "3       3  0.053406  \n",
              "4       1  0.200000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5f7d49ae-0b5b-46e1-8fa3-14a72233a7eb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>topic1</th>\n",
              "      <th>score1</th>\n",
              "      <th>topic2</th>\n",
              "      <th>score2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>investigating emergence particular cell type r...</td>\n",
              "      <td>2</td>\n",
              "      <td>0.406604</td>\n",
              "      <td>4</td>\n",
              "      <td>0.180537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>stimuliresponsive material modify shape respon...</td>\n",
              "      <td>3</td>\n",
              "      <td>0.282587</td>\n",
              "      <td>2</td>\n",
              "      <td>0.253033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>today landscape robotics dominated vertical in...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.247078</td>\n",
              "      <td>0</td>\n",
              "      <td>0.207291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>machine learning model especially based deep a...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.796006</td>\n",
              "      <td>3</td>\n",
              "      <td>0.053406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>study query complexity cake cutting give lower...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f7d49ae-0b5b-46e1-8fa3-14a72233a7eb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5f7d49ae-0b5b-46e1-8fa3-14a72233a7eb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5f7d49ae-0b5b-46e1-8fa3-14a72233a7eb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 279
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berdasarkan output diatas, dapat dilihat bahwa data pertama teridentifikasi sebagai topik 2 sebesar 40,66%, dan juga teridentifikasi sebagai topik 4 sebesar 18,05%, sehingga dapat disimpulkan bahwa data pertama lebih cocok untuk masuk ke dalam topik 2. Begitu juga data yang lainnya. Namun, data terakhir teridentifikasi sebagai topik 0 sebesar 20%, dan juga teridentifikasi sebagai topik 1 sebesar 20%, sehingga data terakhir dapat termasuk ke dalam topik 0, namun juga dapat termasuk ke dalam topik 1 karena memiliki tingkat probabilitas yang sama."
      ],
      "metadata": {
        "id": "NY6LK6wsHyaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nomor 4"
      ],
      "metadata": {
        "id": "XjJKESl8OcSv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Library yang dibutuhkan untuk nomor 4"
      ],
      "metadata": {
        "id": "qo46WeMDJ6Rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx"
      ],
      "metadata": {
        "id": "bO8cef0sgShJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pertama-tama, buat fungsi `read_article` untuk membaca teks dari file dan memisahkannya menjadi kalimat-kalimat terpisah berdasarkan pemisah titik (\".\")"
      ],
      "metadata": {
        "id": "LDBJ4_kpKeoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_article(file_name):\n",
        "    file = open(file_name, \"r\")\n",
        "    filedata = file.readlines()\n",
        "    # Split the sentences based on dot (.) separator\n",
        "    article = filedata[0].split(\". \")\n",
        "    sentences = []\n",
        "    for i, sentence in enumerate(article):\n",
        "        print(f\"{i+1}. {sentence}\")\n",
        "        # Remove non-alphabetic characters\n",
        "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
        "    sentences.pop()\n",
        "\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "jEtiGgV4nJGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lalu, buat fungsi `sentence_similarity` untuk menghitung tingkat kesamaan antara dua kalimat menggunakan pendekatan representasi vektor"
      ],
      "metadata": {
        "id": "E1r_iHSAKwfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        "\n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        "\n",
        "    all_words = list(set(sent1 + sent2))\n",
        "\n",
        "    #build vector 0 with dimension following the len of all_words\n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        "\n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        "\n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        "\n",
        "    return 1 - cosine_distance(vector1, vector2)"
      ],
      "metadata": {
        "id": "_kKHTakdgXLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berikutnya, buat fungsi `build_similarity_matrix` untuk membangun matriks kesamaan antara kalimat-kalimat dalam bentuk array dua dimensi."
      ],
      "metadata": {
        "id": "t-uwfJPmLFaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    # Create an empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "\n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue\n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "\n",
        "    return similarity_matrix"
      ],
      "metadata": {
        "id": "jEjI0SFagZuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Terakhir, buat fungsi `generate_summary` untuk menghasilkan ringkasan teks dari file teks yang diberikan dengan pendekatan extractive serta menampilkan Top 3 Ranked Sentences berdasarkan tingkat similiraty matrix yang dihitung menggunakan fungsi yang sudah dibuat sebelumnya"
      ],
      "metadata": {
        "id": "fL_XQeJoLyTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(file_name, top_n):\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "\n",
        "    print(\"Sentences :\")\n",
        "    # Step 1 - Read text and split it\n",
        "    sentences =  read_article(file_name)\n",
        "\n",
        "    # Step 2 - Generate Similarity Matrix across sentences\n",
        "    sentence_similarity_matrix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity matrix\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "\n",
        "    print(\"\\nIndexes of top ranked sentences:\")\n",
        "    for i, (scores, sentence) in enumerate(ranked_sentences):\n",
        "        print(f\"{i+1}. Score: {scores:.4f} | Sentence: {sentence}\")\n",
        "\n",
        "        if i >= top_n - 1:\n",
        "            break\n",
        "\n",
        "    for i in range(top_n):\n",
        "        summarize_text.append(\" \".join(ranked_sentences[i][1]))\n",
        "\n",
        "    # Step 5 - Output the summarized text\n",
        "    print(\"\\nSummarized Text:\")\n",
        "    for i, summary in enumerate(summarize_text):\n",
        "        print(f\"{i+1}. {summary}\")"
      ],
      "metadata": {
        "id": "kOQDxF7tiP-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_summary('/content/drive/MyDrive/UAS Text Mining/data_4A.txt', 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ppqch_i7gm2G",
        "outputId": "ad61f485-73a9-45c0-c367-cec308befc58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences :\n",
            "1. ﻿Stem cells are cells that (a) on dividing are capable of recreating themselves in at least one daughter cell and (b) have the capacity to differentiate into several linages\n",
            "2. Stem cells play a critical and essential role in the human body not only by providing the starting material for organs and tissues but also for their continual maintenance, growth, and renewal throughout ontogeny [e.g\n",
            "3. hematopoietic stem cells (HSCs) → erythrocytes, neural stem cells (NSCs) → neurons, etc]\n",
            "4. As the embryo and the fetus develop, stem cells are seeded into the various tissues and organs where they remain throughout life (Figure 1)\n",
            "5. Stem cells can also exist temporarily during embryonic development and can be artificially established ex vivo from transient stages of differentiation\n",
            "6. Embryonic stem (ES) cells of mice, for example, are isolated by culturing transiently existing inner cell mass cells of early embryos\n",
            "7. ES cells are cells that retain the properties of primitive ectoderm cells, i.e\n",
            "8. they are undifferentiated but have the capacity to differentiate into all adult tissues\n",
            "9. Because of their potential to provide essential materials for next-generation therapeutics, including gene, cellular, and tissue regeneration therapies, both naturally occurring and artificially established stem cells are the subjects of intense investigation\n",
            "10. The clinical utility of stem cells is limited, however, because no effective technological methodologies exist to cultivate stem cells in vitro, or to stimulate them down particular differentiation pathways\n",
            "11. The development of bioprocesses for the generation or ex vivo maintenance of stem cells and their derivatives is complicated by the biological properties of most stem cell populations\n",
            "12. In general stem cells are rare, quiescent, or slowly cycling cells with complex microenvironmental requirements\n",
            "13. Culture optimization protocols must thus be designed to specifically target stem cell populations while incorporating feedback from a potentially dynamic mature and maturing cell population\n",
            "14. Alternatively, selection or enrichment processes can be used to enhance the frequency of stem cells in hopes of being able to more efficiently target their growth\n",
            "15. Approaches being used to enrich stem cell populations are reviewed elsewhere (1)\n",
            "16. Optimization of culture conditions for stem cell growth and differentiation has been further hindered by the need for retrospective assays of cell function to quantify stem cell responses to candidate culture parameters\n",
            "17. Although these assays have been, and continue to be, exceedingly useful for describing properties of stem cells, their often lengthy and complicated nature represents a significant bottleneck in stem cell bioprocess development\n",
            "18. It is additionally important to realize that the application and interpretation of many of these functional assays depends on established principles that are thought to govern the dynamics of all stem cell populations\n",
            "19. These principles include (a) the irreversibility of stem cell differentiation, (b) the progressive commitment to a particular lineage, and (c) a hierarchical framework with increasing numbers of cells with a progressively decreasing capacity for self-renewal, proliferation, and lineage potential\n",
            "20. Although, in general, these principles have been shown to be true, recent studies that challenge some of these concepts (e.g\n",
            "21. 2–6) stress the need for the continual reevaluation of established dogma (7)\n",
            "22. The purpose of this review is to outline the salient biological properties of stem cell populations, highlight the main bioengineering challenges in the development of stem cell–based technologies, and focus on how recent information about parameters that may influence stem cell self-renewal and differentiation impact the design of these bioprocesses\n",
            "23. In addressing these questions, we draw mainly on results from embryogenesis and hematopoiesis, although examples from neurogenesis and other stem cell systems are used as appropriate to argue the generality of our observations\n",
            "24. Given the breadth of the information available, an exhaustive analysis of all potential cell systems and culture parameters is not feasible (or desirable)\n",
            "25. Our conceptual focus thus represents our perspective on future directions that we believe may be fruitful in addressing the critical bioengineering challenges that must be overcome to develop clinically relevant stem cell–based technologies.\n",
            "\n",
            "Indexes of top ranked sentences:\n",
            "1. Score: 0.0674 | Sentence: ['The', 'development', 'of', 'bioprocesses', 'for', 'the', 'generation', 'or', 'ex', 'vivo', 'maintenance', 'of', 'stem', 'cells', 'and', 'their', 'derivatives', 'is', 'complicated', 'by', 'the', 'biological', 'properties', 'of', 'most', 'stem', 'cell', 'populations']\n",
            "2. Score: 0.0573 | Sentence: ['hematopoietic', 'stem', 'cells', '(HSCs)', '→', 'erythrocytes,', 'neural', 'stem', 'cells', '(NSCs)', '→', 'neurons,', 'etc]']\n",
            "3. Score: 0.0571 | Sentence: ['Optimization', 'of', 'culture', 'conditions', 'for', 'stem', 'cell', 'growth', 'and', 'differentiation', 'has', 'been', 'further', 'hindered', 'by', 'the', 'need', 'for', 'retrospective', 'assays', 'of', 'cell', 'function', 'to', 'quantify', 'stem', 'cell', 'responses', 'to', 'candidate', 'culture', 'parameters']\n",
            "\n",
            "Summarized Text:\n",
            "1. The development of bioprocesses for the generation or ex vivo maintenance of stem cells and their derivatives is complicated by the biological properties of most stem cell populations\n",
            "2. hematopoietic stem cells (HSCs) → erythrocytes, neural stem cells (NSCs) → neurons, etc]\n",
            "3. Optimization of culture conditions for stem cell growth and differentiation has been further hindered by the need for retrospective assays of cell function to quantify stem cell responses to candidate culture parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output diatas merupakan 3 kalimat terpenting yang dapat merepresentasikan keseluruhan teks beserta dengan nilai similarity matrixnya"
      ],
      "metadata": {
        "id": "AG02o0PcMRVx"
      }
    }
  ]
}